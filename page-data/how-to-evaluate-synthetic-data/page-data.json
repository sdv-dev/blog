{"componentChunkName":"component---src-templates-post-js","path":"/how-to-evaluate-synthetic-data/","result":{"data":{"ghostPost":{"id":"Ghost__Post__633b5bbbda16fc003d4eab71","title":"How to evaluate synthetic data for your project — and avoid the biggest mistake we see","slug":"how-to-evaluate-synthetic-data","featured":true,"feature_image":"https://sdv.ghost.io/content/images/2022/10/Header-V2.png","excerpt":"Proper evaluation is critical when using synthetic data. Avoid this common mistake and lead your project to success.","custom_excerpt":"Proper evaluation is critical when using synthetic data. Avoid this common mistake and lead your project to success.","visibility":"public","created_at_pretty":"03 October, 2022","published_at_pretty":"07 October, 2022","updated_at_pretty":"10 January, 2023","created_at":"2022-10-03T18:01:31.000-04:00","published_at":"2022-10-07T10:24:15.000-04:00","updated_at":"2023-01-10T13:00:46.000-05:00","meta_title":"How to evaluate synthetic data for your project","meta_description":"Proper evaluation is critical when using synthetic data. Avoid this common mistake and lead your project to success.","og_description":null,"og_image":null,"og_title":null,"twitter_description":"Proper evaluation is critical when using synthetic data. Avoid this common mistake and lead your project to success.","twitter_image":null,"twitter_title":"How to evaluate synthetic data for your project","authors":[{"name":"Neha Patki","slug":"neha","bio":"Neha first created the SDV for her Master's thesis at MIT and also has experience in Product Management from Google. She is excited to use her expertise to build a great user experience at DataCebo.","profile_image":"https://sdv.ghost.io/content/images/2021/05/Neha_Patki--1-.jpg","twitter":"@n4atki","facebook":null,"website":"https://www.linkedin.com/in/nehapatki/"}],"primary_author":{"name":"Neha Patki","slug":"neha","bio":"Neha first created the SDV for her Master's thesis at MIT and also has experience in Product Management from Google. She is excited to use her expertise to build a great user experience at DataCebo.","profile_image":"https://sdv.ghost.io/content/images/2021/05/Neha_Patki--1-.jpg","twitter":"@n4atki","facebook":null,"website":"https://www.linkedin.com/in/nehapatki/"},"primary_tag":{"name":"Product","slug":"product","description":"Our product blog provides insights and tips into creating and deploying high quality synthetic data. We'll support our findings with experiments, illustrations and tutorials.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Product","slug":"product","description":"Our product blog provides insights and tips into creating and deploying high quality synthetic data. We'll support our findings with experiments, illustrations and tutorials.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"In recent years, synthetic data has shown great promise for solving a variety of\nproblems – like addressing data scarcity for AI\n[https://www.gartner.com/en/newsroom/press-releases/2022-06-22-is-synthetic-data-the-future-of-ai] \nand overcoming barriers to data access\n[https://www.agmatix.com/blog/driving-innovation-in-agriculture-with-synthetic-data/?utm_source=LinkedIn&utm_medium=Social]\n. As your organization becomes serious about adopting synthetic data, it's\ncrucial to incorporate the right metrics and evaluation frameworks into your\nprojects.\n\nSince the synthetic data space is so new, there aren't yet industry standards\nfor setting and measuring outcomes. At DataCebo [https://datacebo.com/], we've\nworked with a variety of teams using synthetic data. In this article, we're\nsharing the best practices we've learned along the way, as well as the one key\nmistake to avoid.\n\nWhat are synthetic data metrics?\nIn some fields – such as synthetic image generation – it's easy to visually\ninspect the output (in this case, synthetic images) and determine its quality.\nBut if you are creating synthetic data in a tabular format (with rows and\ncolumns), it's difficult to make an overall assessment just by looking at the\nraw data. This is evident in the table below:\n\nWhen the synthetic data is tabular, it's hard to assess its quality. In this\nexample, 3 of the rows show data from real students, while the other 3 are\nsynthetically created. Can you tell which is which?For tabular synthetic data,\nit's necessary to create metrics that quantify how the synthetic data compares\nto the real data. Each metric measures a particular aspect of the data – such as\ncoverage or correlation – allowing you to identify which specific elements have\nbeen preserved or forgotten during the synthetic data process.\n\nIn our open source library, SDMetrics [https://github.com/sdv-dev/SDMetrics],\nwe've provided a variety of metrics for evaluating synthetic data against the\nreal data. For instance, you can use the CategoryCoverage\n[https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/categorycoverage] and \nRangeCoverage\n[https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/rangecoverage] metrics\nto quantify whether your synthetic data covers the same range of possible values\nas the real data:\n\nIn this example, the numerical distributions of real and synthetic data are\noverlaid to compare coverage. Using SDMetrics, you can apply the RangeCoverage,\nwhich quantifies the coverage. In this case: 82%.You may also be curious about\nwhether the synthetic data captures trends between pairs of columns. To compare\ncorrelations, you can use the CorrelationSimilarity\n[https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/correlationsimilarity] \nmetric:\n\nThis example shows two side-by-side heatmaps of the pairwise correlations for\nreal and synthetic data. Using SDMetrics, you can apply the\nCorrelationSimilarity metric, which quantifies the similarity as a score from 0\nto 1.The SDMetrics library has over 30 metrics, with more still in development.\n\nBut having access to metrics is just one part of the story. With so many\nmetrics, it can be difficult to decide which ones to focus on – and how to make\nprogress in your synthetic data project. To successfully deploy synthetic data,\nit's important to consider metrics during all steps of your project development\ncycle.\n\nIn the rest of this article, we'll share a 3-step plan for incorporating metrics\n– and the SDMetrics library – into your synthetic data project to increase your\nchances of success. \n\nStep 1: Start with the project goals \nIt is tempting to create synthetic data quickly and then test it using all the\navailable metrics. After all, it's hard not to be curious about what synthetic\ndata can do! But to succeed with your project, it's important to take a step\nback and focus on the problem you are trying to solve first.\n\nSynthetic data creation isn't an end in itself. Just as with most data work, you\ndon't create synthetic data for its own sake — you use it to solve a problem. If\nyou want your synthetic data project to succeed, pay close attention to what\nthat problem is, as this will help you narrow down a few key metrics. \n\nFor example, imagine that your organization has two different synthetic data\nprojects related to software testing and machine learning, respectively. Because\nthese projects have different goals, you’ll need to consider different metrics:\n\nUse your project goals to help you decide which metrics to prioritize.Focusing\non your goals allows you to identify which metrics are important for the\nultimate success of your project. The single biggest mistake we see people\nmaking is to skip this critical step. Without focus, you can easily get bogged\ndown running multiple tests and tweaking your synthetic dataset, rather than\nmeeting the specific considerations for your project. This can derail your\nefforts – leading to a complex project that doesn't add any value.\n\nStep 2: Let your goals guide the synthetic data creation \nYour goals can help you appropriately scope your project and cut costs. A core\nsubset of metrics can guide your synthetic data creation, making it faster and\nmore targeted to your needs.\n\nChances are, you'll be faced with many decisions throughout your project. For\nexample, in the SDV library [https://github.com/sdv-dev/SDV], there are 5\ndifferent algorithms that create synthetic data, each with their own settings\nthat lead to hundreds of potential models you can create. But if you know that\nyour synthetic data project is software testing, you've identified that\ncoverage, boundaries and business rules are the highest priority metrics. This\nwill guide your decision-making.\n\nIn this case, you may find success choosing our preset model, FAST ML\n[https://github.com/sdv-dev/SDV/discussions/786]. This model uses statistical\nmethods to achieve your minimal requirements while also providing high\nperformance – FAST ML can train a mid-size data table (100 columns and 100K\nrows) in only a few minutes. You can compare this to other GAN-based models that\nare more resource-intensive, taking hours to finish. If your project metrics are\nsatisfied with FAST ML, it is reasonable to choose this model over a GAN, even\nif it isn't perfectly optimized across all possible metrics.\n\nFrom this example, we can see that metrics are not just something to evaluate at\nthe end of the project – they are useful tools for decision-making throughout \nyour project. \n\nStep 3: Test the end-to-end workflow upfront\nThe purpose of metrics is to provide guardrails and focus for your project –\nscoping it so that you can drive business value most efficiently. For the\nhighest chances of success, it's important to apply the synthetic data\nend-to-end for downstream applications, so that you can verify that business\nvalue upfront.\n\nContinuing with our example of software testing, it's important to use the\nsynthetic data for your downstream software testing suite as quickly as you can\nto verify the benefits of synthetic data. If you've chosen your metrics\ncorrectly and considered them when making decisions (steps #1 and #2), then\nyou'll see that this translates to business value.\n\nGoing end-to-end means applying the synthetic data and verifying the ultimate\nbusiness value it provides.This type of cost-benefit analysis can help you make\nthe case for synthetic data adoption within your enterprise. It can also help\nyou iterate – to get more business value from your synthetic data, you can\ncontinue to optimize the metrics you've chosen in step #1, or identify new ones.\n\nThe Takeaway\nTechnically, there are an infinite number of metrics that could be used to\nevaluate synthetic data. The key to success is to incorporate select metrics\ninto your synthetic data project development rather than just applying all\nmetrics at the end.\n\nYour project goals are critical to helping you choose the right metrics. Setting\nthem upfront allows you to make better decisions during your project\ndevelopment. And going end-to-end allows you to measure the business value that\nyour synthetic data brings to the organization.\n\nWhat are your thoughts? Leave comments below! If you noticed other evaluation\npitfalls in your projects, let us know below or reach us directly at \ninfo@sdv.dev.","html":"<p>In recent years, synthetic data has shown great promise for solving a variety of problems – like <a href=\"https://www.gartner.com/en/newsroom/press-releases/2022-06-22-is-synthetic-data-the-future-of-ai\">addressing data scarcity for AI</a> and <a href=\"https://www.agmatix.com/blog/driving-innovation-in-agriculture-with-synthetic-data/?utm_source=LinkedIn&amp;utm_medium=Social\">overcoming barriers to data access</a>. As your organization becomes serious about adopting synthetic data, it's crucial to incorporate the right metrics and evaluation frameworks into your projects.</p><p>Since the synthetic data space is so new, there aren't yet industry standards for setting and measuring outcomes. At <a href=\"https://datacebo.com/\">DataCebo</a>, we've worked with a variety of teams using synthetic data. In this article, we're sharing the best practices we've learned along the way, as well as the one key mistake to avoid.</p><h3 id=\"what-are-synthetic-data-metrics\">What are synthetic data metrics?</h3><p>In some fields – such as synthetic image generation – it's easy to visually inspect the output (in this case, synthetic images) and determine its quality. But if you are creating synthetic data in a tabular format (with rows and columns), it's difficult to make an overall assessment just by looking at the raw data. This is evident in the table below:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/10/Real-vs.-Synthetic-Data.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"667\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/10/Real-vs.-Synthetic-Data.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/10/Real-vs.-Synthetic-Data.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/10/Real-vs.-Synthetic-Data.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/10/Real-vs.-Synthetic-Data.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>When the synthetic data is tabular, it's hard to assess its quality. In this example, 3 of the rows show data from real students, while the other 3 are synthetically created. Can you tell which is which?</em></figcaption></figure><p>For tabular synthetic data, it's necessary to create metrics that quantify how the synthetic data compares to the real data. Each metric measures a particular aspect of the data – such as coverage or correlation – allowing you to identify which specific elements have been preserved or forgotten during the synthetic data process.</p><p>In our open source library, <a href=\"https://github.com/sdv-dev/SDMetrics\">SDMetrics</a>, we've provided a variety of metrics for evaluating synthetic data against the real data. For instance, you can use the <a href=\"https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/categorycoverage\">CategoryCoverage</a> and <a href=\"https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/rangecoverage\">RangeCoverage</a> metrics to quantify whether your synthetic data covers the same range of possible values as the real data:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/10/Range-Coverage.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"645\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/10/Range-Coverage.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/10/Range-Coverage.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/10/Range-Coverage.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/10/Range-Coverage.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>In this example, the numerical distributions of real and synthetic data are overlaid to compare coverage. Using SDMetrics, you can apply the RangeCoverage, which quantifies the coverage. In this case: 82%.</em></figcaption></figure><p>You may also be curious about whether the synthetic data captures trends between pairs of columns. To compare correlations, you can use the <a href=\"https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/correlationsimilarity\">CorrelationSimilarity</a> metric:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/10/Column-Pairs.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"645\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/10/Column-Pairs.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/10/Column-Pairs.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/10/Column-Pairs.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/10/Column-Pairs.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>This example shows two side-by-side heatmaps of the pairwise correlations for real and synthetic data. Using SDMetrics, you can apply the CorrelationSimilarity metric, which quantifies the similarity as a score from 0 to 1.</em></figcaption></figure><p>The SDMetrics library has over 30 metrics, with more still in development.</p><p>But having access to metrics is just one part of the story. With so many metrics, it can be difficult to decide which ones to focus on – and how to make progress in your synthetic data project. <strong>To successfully deploy synthetic data, it's important to consider metrics during all steps of your project development cycle.</strong></p><p>In the rest of this article, we'll share a 3-step plan for incorporating metrics – and the SDMetrics library – into your synthetic data project to increase your chances of success. </p><h3 id=\"step-1-start-with-the-project-goals\">Step 1: Start with the project goals </h3><p>It is tempting to create synthetic data quickly and then test it using all the available metrics. After all, it's hard not to be curious about what synthetic data can do! But to succeed with your project, it's important to take a step back and focus on the problem you are trying to solve first.</p><p>Synthetic data creation isn't an end in itself. Just as with most data work, you don't create synthetic data for its own sake — you use it to solve a problem. If you want your synthetic data project to succeed, pay close attention to what that problem is, as this will help you narrow down a few key metrics. </p><p>For example, imagine that your organization has two different synthetic data projects related to software testing and machine learning, respectively. Because these projects have different goals, you’ll need to consider different metrics:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/10/Project-Driven-Metrics.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"667\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/10/Project-Driven-Metrics.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/10/Project-Driven-Metrics.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/10/Project-Driven-Metrics.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/10/Project-Driven-Metrics.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Use your project goals to help you decide which metrics to prioritize.</figcaption></figure><p>Focusing on your goals allows you to identify which metrics are important for the ultimate success of your project. <strong>The single biggest mistake we see people making is to skip this critical step.</strong> Without focus, you can easily get bogged down running multiple tests and tweaking your synthetic dataset, rather than meeting the specific considerations for your project. This can derail your efforts – leading to a complex project that doesn't add any value.</p><h3 id=\"step-2-let-your-goals-guide-the-synthetic-data-creation\">Step 2: Let your goals guide the synthetic data creation </h3><p>Your goals can help you appropriately scope your project and cut costs. A core subset of metrics can guide your synthetic data creation, making it faster and more targeted to your needs.</p><p>Chances are, you'll be faced with many decisions throughout your project. For example, in the <a href=\"https://github.com/sdv-dev/SDV\">SDV library</a>, there are 5 different algorithms that create synthetic data, each with their own settings that lead to hundreds of potential models you can create. But if you know that your synthetic data project is software testing, you've identified that coverage, boundaries and business rules are the highest priority metrics. This will guide your decision-making.</p><p>In this case, you may find success choosing our preset model, <a href=\"https://github.com/sdv-dev/SDV/discussions/786\">FAST ML</a>. This model uses statistical methods to achieve your minimal requirements while also providing high performance – FAST ML can train a mid-size data table (100 columns and 100K rows) in only a few minutes. You can compare this to other GAN-based models that are more resource-intensive, taking hours to finish. If your project metrics are satisfied with FAST ML, it is reasonable to choose this model over a GAN, even if it isn't perfectly optimized across all possible metrics.</p><p>From this example, we can see that metrics are not just something to evaluate at the end of the project – they are useful tools for decision-making <em>throughout</em> your project. </p><h3 id=\"step-3-test-the-end-to-end-workflow-upfront\">Step 3: Test the end-to-end workflow upfront</h3><p>The purpose of metrics is to provide guardrails and focus for your project – scoping it so that you can drive business value most efficiently. For the highest chances of success, it's important to apply the synthetic data end-to-end for downstream applications, so that you can verify that business value upfront.</p><p>Continuing with our example of software testing, it's important to use the synthetic data for your downstream software testing suite as quickly as you can to verify the benefits of synthetic data. If you've chosen your metrics correctly and considered them when making decisions (steps #1 and #2), then you'll see that this translates to business value.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/10/Business-Value-Driven-Metrics.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"667\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/10/Business-Value-Driven-Metrics.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/10/Business-Value-Driven-Metrics.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/10/Business-Value-Driven-Metrics.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/10/Business-Value-Driven-Metrics.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Going end-to-end means applying the synthetic data and verifying the ultimate business value it provides.</figcaption></figure><p>This type of cost-benefit analysis can help you make the case for synthetic data adoption within your enterprise. It can also help you iterate – to get more business value from your synthetic data, you can continue to optimize the metrics you've chosen in step #1, or identify new ones.</p><h3 id=\"the-takeaway\">The Takeaway</h3><p>Technically, there are an infinite number of metrics that could be used to evaluate synthetic data. The key to success is to incorporate select metrics into your synthetic data project development rather than just applying all metrics at the end.</p><p>Your project goals are critical to helping you choose the right metrics. Setting them upfront allows you to make better decisions during your project development. And going end-to-end allows you to measure the business value that your synthetic data brings to the organization.</p><p><strong>What are your thoughts? </strong>Leave comments below! If you noticed other evaluation pitfalls in your projects, let us know below or reach us directly at <a href=\"mailto:info@sdv.dev\">info@sdv.dev</a>.</p>","url":"https://sdv.ghost.io/how-to-evaluate-synthetic-data/","canonical_url":null,"uuid":"1cbbc888-463b-4494-8e88-a71353ac697a","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"633b5bbbda16fc003d4eab71","reading_time":6}},"pageContext":{"slug":"how-to-evaluate-synthetic-data"}},"staticQueryHashes":["2061773391","2358152166","2362887240","2439066133","2561578252","2657115718","2731221146","2839364760","4145280475"]}