{"componentChunkName":"component---src-templates-tag-js","path":"/tag/product/","result":{"data":{"ghostTag":{"slug":"product","name":"Product","visibility":"public","feature_image":null,"description":"Our product blog provides insights and tips into creating and deploying high quality synthetic data. We'll support our findings with experiments, illustrations and tutorials.","meta_title":null,"meta_description":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__63a0bc44ac52ed003d6a169a","title":"Interpreting the Progress of CTGAN","slug":"interpreting-ctgan-progress","featured":false,"feature_image":"https://sdv.ghost.io/content/images/2022/12/Header--4-.png","excerpt":"It can be difficult to verify the progress that a GAN is making. What if we combined it with easily interpretable metrics and visualizations?","custom_excerpt":"It can be difficult to verify the progress that a GAN is making. What if we combined it with easily interpretable metrics and visualizations?","visibility":"public","created_at_pretty":"19 December, 2022","published_at_pretty":"20 December, 2022","updated_at_pretty":"23 January, 2023","created_at":"2022-12-19T14:32:20.000-05:00","published_at":"2022-12-20T14:13:29.000-05:00","updated_at":"2023-01-23T17:09:24.000-05:00","meta_title":"Interpreting the Progress of CTGAN","meta_description":"It can be difficult to verify the progress that a GAN is making. What if we combined it with easily interpretable metrics and visualizations?","og_description":null,"og_image":null,"og_title":null,"twitter_description":"It can be difficult to see the progress of a GAN. What if we verify it with metrics and visualizations?","twitter_image":null,"twitter_title":"Interpreting the Progress of CTGAN","authors":[{"name":"Santiago Gomez Paz","slug":"santiago","bio":"Santiago is a Sophomore at BYU and an aspiring entrepreneur who spent a summer interning at DataCebo, learning and experimenting with CTGAN.","profile_image":"https://sdv.ghost.io/content/images/2022/10/Santi_Gomez_Paz.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Santiago Gomez Paz","slug":"santiago","bio":"Santiago is a Sophomore at BYU and an aspiring entrepreneur who spent a summer interning at DataCebo, learning and experimenting with CTGAN.","profile_image":"https://sdv.ghost.io/content/images/2022/10/Santi_Gomez_Paz.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Product","slug":"product","description":"Our product blog provides insights and tips into creating and deploying high quality synthetic data. We'll support our findings with experiments, illustrations and tutorials.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Product","slug":"product","description":"Our product blog provides insights and tips into creating and deploying high quality synthetic data. We'll support our findings with experiments, illustrations and tutorials.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"This article was researched by Santiago Gomez Paz, a DataCebo intern. Santiago is a Sophomore at BYU and an aspiring entrepreneur who spent his summer learning and experimenting with CTGAN.\n\nThe open source SDV library offers many options for creating synthetic data tables. Some of the library's models use tried-and-true methods from classical statistics, while others use newer innovations like deep learning. One of the newest and most popular models is CTGAN, which uses a type of neural network called a Generative Adversarial Network (GAN).\n\nGenerative models are a popular choice for creating all kinds of synthetic data – for example, you may have heard of OpenAI's DALL-E or ChatGPT tools, which use trained models to create synthetic images and text respectively. A large driver behind their popularity is that they work well — they create synthetic data that closely resembles the real deal. But this high quality often comes at a cost.\n\nGenerative models can be resource-intensive. It can take a lot of time to properly train one, and it's not always clear whether the model is improving much during the training process.\n\nIn this article, we'll unpack this complexity by performing experiments on CTGAN. We'll cover –\n\n * A high-level explanation of how GANs work\n * How to measure and interpret the progress of CTGAN\n * How to confirm this progress with more interpretable, user-centric metrics\n\nSince the library is open source, you can see and run the code yourself with this Colab Notebook.\n\n\nHow do GANs work?\n\nBefore we begin, it's important to understand how GANs work. At a high level, a GAN is an algorithm that makes two neural networks compete against each other (thus the label “Adversarial”). These neural networks are known as the generator and the discriminator, and they each have competing goals:\n\n * The discriminator's goal is to tell real data apart from synthetic data\n * The generator's goal is to create synthetic data that fools the discriminator\n\nThe setup is illustrated below.\n\nThis setup allows us to measure – and improve – both neural networks over many iterations by telling them what they got wrong. Each of these iterations is called an epoch, and CTGAN tracks inaccuracies as loss values. The neural networks are trying to minimize their loss values for every epoch.\n\nThe CTGAN algorithm calculates loss values using a specific formula that can be found in this discussion. The intuition behind it is shown below.\n\nAs shown by the table, lower loss values – even if they are negative – mean that the neural networks are doing well.\n\nAs the epochs progress, we expect both neural networks to improve at their respective goals – but each epoch is resource-intensive and takes time to run. A common request is to find a tradeoff between the improvement achieved and the resources used.\n\n\nMeasuring progress using CTGAN\n\nThe open source SDV library makes it easy to train a CTGAN model and inspect its progress. The code below shows the steps. We train CTGAN using a publicly available SDV demo dataset named RacketSports, which stores various measurements of the strokes that tennis and squash players make over the course of a game.\n\nfrom sdv.demo import load_tabular_demo\nfrom sdv.tabular import CTGAN\n\nmetadata, real_data = load_tabular_demo('RacketSports', metadata=True)\ntable_metadata = metadata.to_dict()\n\nmodel = CTGAN(table_metadata, verbose=True, epochs=800)\nmodel.fit(real_data)\n\nAs part of the fitting process, CTGAN trains the neural networks for multiple epochs. After each epoch, it prints out the count, the generator loss (G) and the discriminator loss (D). Keep in mind that lower numbers are better – even if they are negative. An example is shown below.\n\nEpoch 1, Loss G:  1.0435,Loss D: -0.1401\nEpoch 2, Loss G:  0.4489,Loss D: -0.1455\nEpoch 3, Loss G:  0.4756,Loss D: -0.0956\nEpoch 4, Loss G:  0.3902,Loss D:  0.0344\nEpoch 5, Loss G:  0.0912,Loss D:  0.3030\n...\n\nTo see how the neural networks are improving, we plot the loss values for every epoch. The results from our experiment are shown in the graph below.\n\nBased on the characteristics of this graph, it's possible to deduce how the GAN is progressing.\n\n\nInterpreting the loss values\n\nThe graph above may seem confusing at first glance: Why is the discriminator's loss value score oscillating at 0 if it is supposed to improve (minimize and become negative) over time? The key to interpreting the loss values is to remember that the neural networks are adversaries. As one improves, the other must also improve just to keep its score consistent. Here are three scenarios that we frequently see:\n\n 1. Generator loss is slightly positive while discriminator loss is 0. This means that the generator is producing poor quality synthetic data while the discriminator is blindly guessing what is real vs. synthetic. This is a common starting point, where neither neural network has optimized for its goal.\n 2. Generator loss is becoming negative while the discriminator loss remains at 0. This means that the generator is producing better and better synthetic data. The discriminator is improving too, but because the synthetic data quality has increased, it is still unable to clearly differentiate real vs. synthetic data.\n 3. Generator loss has stabilized at a negative value while the discriminator loss remains at 0. This means that the generator has optimized, creating synthetic data that looks so real, the discriminator cannot tell it apart.\n\nIt is encouraging to see that the general pattern for the RacketSports dataset is similar to a variety of other datasets. These are shown below.\n\nOf course, other patterns may be possible for different datasets. But if loss values are not stabilizing, watch out! This would indicate that the neural networks were not able to effectively learn patterns in the real data.\n\n\nMetrics-Powered Analysis\n\nYou may be wondering whether to trust the loss values. Do they indicate a meaningful difference in synthetic data quality? To answer this question, it's helpful to create synthetic data sets after training the model for different numbers of epochs, and assess the quality of the data sets.\n\nNUM_SYNTHETIC_ROWS = len(real_data)\n\nsynthetic_data = model.sample(num_rows=NUM_SYNTHETIC_ROWS)\n\nIt is important to select a few key metrics for a quantifiable quality measure. For our experiments, we chose 4 metrics from the open source SDMetrics library:\n\n * KSComplement evaluates the shape of numerical columns\n * TVComplement evaluates the shape of discrete columns\n * CorrelationSimilarity evaluates pairwise correlations between columns\n * CategoryCoverage evaluates whether the synthetic data covers all possible values\n\nEach metric produces a score ranging from 0 (worst quality) to 1 (best quality). In the example below, we use the KSComplement metric on a numerical column in the RacketSports dataset.\n\nfrom sdmetrics.single_column import KSComplement\n\nNUMERICAL_COLUMN_NAME='dim_2'\n\nscore = KSComplement.compute(\n   real_data[NUMERICAL_COLUMN_NAME],\n   synthetic_data[NUMERICAL_COLUMN_NAME])\n\nOur results validate that the scores do, indeed, correlate with the loss value from the generator: The quality improves as the loss is minimized. Some of the metrics – such as CorrelationSimilarity and CategoricalCoverage – are high to begin with, so there is not much room to improve. But other metrics, like KSComplement, show significant improvement. This is shown in the graph below.\n\nIt's also possible to visualize the synthetic data that corresponds to a specific metric. For example, KSComplement compares the overall shape of a real and a synthetic data column, so we can visualize it using histograms.\n\nfrom sdmetrics.reports import utils\n\nutils.get_column_plot(\n  real_data,\n  synthetic_data,\n  column_name=NUMERICAL_COLUMN_NAME,\n  metadata=table_metadata)\n\nOverall, we can conclude that the generator and discriminator losses correspond to the quality metrics that we measured – which means we can trust the loss values, as well as the synthetic data that our CTGAN created!\n\n\nConclusion\n\nIn this article, we explored the improvements that the CTGAN model makes as it iterates over many epochs. We started by interpreting the loss values that each of the neural networks – the generator and the discriminator – reports over time. This helped us reason about how they were progressing. But to fully trust the progress of our model, we then turned to the SDMetrics library, which provides metrics that are easier to interpret. Using this library, we could verify whether the reported loss values truly resulted in synthetic data quality improvements.\n\nThis may lead us to a new, potential feature: What if we integrated these easily interpretable, user-centric metrics into the CTGAN training progress? This feature would allow you to specify the exact metrics you'd like to optimize upfront – for example, KSComplement. In addition to the generator and discriminator loss, CTGAN may be able to report a snapshot of this metric. A hypothetical example is shown below.\n\nmodel = CTGAN(\n  table_metadata,\n  verbose=True,\n  epochs=800,\n  optimization_metric='KSComplement',\n  optimization_column='dim_2')\n  \nmodel.fit(real_data)\n\nEpoch 1, Loss G: 1.0435, Loss D: -0.1401, KSComplement: 0.7832\nEpoch 2, Loss G: 0.4489, Loss D: -0.1455, KSComplement: 0.7671\nEpoch 3, Loss G: 0.4756, Loss D: -0.0956, KSComplement: 0.7664\n…\nEpoch 200: Loss G: -2.542, Loss D: 0.0002911, KSComplement: 0.92391\n\n\nSuch a feature would allow more transparency over CTGAN's learning process, and allow you to stop training your models once the metrics are high.\n\nWhat do you think? If you're interested in exploring the inner workings of CTGAN and optimizing your synthetic data, drop us a comment below!","html":"<p><em>This article was researched by Santiago Gomez Paz, a DataCebo intern. Santiago is a Sophomore at BYU and an aspiring entrepreneur who spent his summer learning and experimenting with CTGAN.</em></p><p>The <a href=\"https://github.com/sdv-dev/SDV\">open source SDV library</a> offers many options for creating synthetic data tables. Some of the library's models use tried-and-true methods from classical statistics, while others use newer innovations like deep learning. One of the newest and most popular models is <strong>CTGAN</strong>, which uses a type of neural network called a Generative Adversarial Network (GAN). </p><p>Generative models are a popular choice for creating all kinds of synthetic data – for example, you may have heard of <a href=\"https://openai.com/dall-e-2/\">OpenAI's DALL-E</a> or <a href=\"https://openai.com/blog/chatgpt/\">ChatGPT</a> tools, which use trained models to create synthetic images and text respectively. A large driver behind their popularity is that they work well — they create synthetic data that closely resembles the real deal. But this high quality often comes at a cost.</p><p>Generative models can be resource-intensive. It can take a lot of time to properly train one, and it's not always clear whether the model is improving much during the training process. </p><p>In this article, we'll unpack this complexity by performing experiments on CTGAN. We'll cover –</p><ul><li>A high-level explanation of how GANs work</li><li>How to measure and interpret the progress of CTGAN</li><li>How to confirm this progress with more interpretable, user-centric metrics</li></ul><p>Since the library is open source, you can see and run the code yourself with this <a href=\"https://colab.research.google.com/drive/1RbIYxkbPP3JQY7W0S1p_XprY25wOYTPL?usp=sharing\">Colab Notebook</a>.</p><h3 id=\"how-do-gans-work\">How do GANs work?</h3><p>Before we begin, it's important to understand how GANs work. At a high level, a GAN is an algorithm that makes two neural networks compete against each other (thus the label “Adversarial”). These neural networks are known as the <strong>generator</strong> and the <strong>discriminator</strong>, and they each have competing goals:</p><ul><li>The discriminator's goal is to tell real data apart from synthetic data</li><li>The generator's goal is to create synthetic data that fools the discriminator</li></ul><p>The setup is illustrated below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2023/01/How-a-GAN-works-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2023/01/How-a-GAN-works-2.png 600w, https://sdv.ghost.io/content/images/size/w1000/2023/01/How-a-GAN-works-2.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2023/01/How-a-GAN-works-2.png 1600w, https://sdv.ghost.io/content/images/2023/01/How-a-GAN-works-2.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>The <strong>generator</strong> is a neural network that creates synthetic data. In this case, it creates a table describing the names of different people, along with their heights and ages. The <strong>discriminator</strong> is an adversarial network that tries to tell these synthetic people apart from the real ones.</em></figcaption></figure><p>This setup allows us to measure – and improve – both neural networks over many iterations by telling them what they got wrong. Each of these iterations is called an <strong>epoch</strong>, and CTGAN tracks inaccuracies as <strong>loss values</strong>. The neural networks are trying to minimize their loss values for every epoch.</p><p>The CTGAN algorithm calculates loss values using a specific formula that can be found in <a href=\"https://github.com/sdv-dev/SDV/discussions/980\">this discussion</a>. The intuition behind it is shown below.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://sdv.ghost.io/content/images/2023/01/Loss-Values-Interpretation-2.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"800\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2023/01/Loss-Values-Interpretation-2.png 600w, https://sdv.ghost.io/content/images/size/w1000/2023/01/Loss-Values-Interpretation-2.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2023/01/Loss-Values-Interpretation-2.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2023/01/Loss-Values-Interpretation-2.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>As shown by the table, lower loss values – even if they are <em>negative </em>– mean that the neural networks are doing well.</p><p>As the epochs progress, we expect both neural networks to improve at their respective goals – but each epoch is resource-intensive and takes time to run. A common request is to find a tradeoff between the improvement achieved and the resources used.</p><h3 id=\"measuring-progress-using-ctgan\">Measuring progress using CTGAN</h3><p>The open source SDV library makes it easy to train a CTGAN model and inspect its progress. The code below shows the steps. We train CTGAN using a publicly available SDV demo dataset named <code>RacketSports</code>, which stores various measurements of the strokes that tennis and squash players make over the course of a game.</p><pre><code class=\"language-python\">from sdv.demo import load_tabular_demo\nfrom sdv.tabular import CTGAN\n\nmetadata, real_data = load_tabular_demo('RacketSports', metadata=True)\ntable_metadata = metadata.to_dict()\n\nmodel = CTGAN(table_metadata, verbose=True, epochs=800)\nmodel.fit(real_data)</code></pre><p>As part of the fitting process, CTGAN trains the neural networks for multiple epochs. After each epoch, it prints out the count, the generator loss (G) and the discriminator loss (D). Keep in mind that lower numbers are better – even if they are <em>negative</em>. An example is shown below.</p><pre><code>Epoch 1, Loss G:  1.0435,Loss D: -0.1401\nEpoch 2, Loss G:  0.4489,Loss D: -0.1455\nEpoch 3, Loss G:  0.4756,Loss D: -0.0956\nEpoch 4, Loss G:  0.3902,Loss D:  0.0344\nEpoch 5, Loss G:  0.0912,Loss D:  0.3030\n...</code></pre><p>To see how the neural networks are improving, we plot the loss values for every epoch. The results from our experiment are shown in the graph below. </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/12/Racket-Sports-Loss.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"645\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/12/Racket-Sports-Loss.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/12/Racket-Sports-Loss.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/12/Racket-Sports-Loss.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/12/Racket-Sports-Loss.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>A graph of the GAN's progress over time. The generator loss is shown in blue, while the discriminator loss for the same epoch is shown in red.</em></figcaption></figure><p>Based on the characteristics of this graph, it's possible to deduce how the GAN is progressing.</p><h3 id=\"interpreting-the-loss-values\">Interpreting the loss values</h3><p>The graph above may seem confusing at first glance: Why is the discriminator's loss value score oscillating at 0 if it is supposed to improve (minimize and become negative) over time? The key to interpreting the loss values is to remember that the neural networks are adversaries. As one improves, the other must also improve just to keep its score consistent. Here are three scenarios that we frequently see:</p><ol><li><strong>Generator loss is slightly positive while discriminator loss is 0. </strong>This means that the generator is producing poor quality synthetic data while the discriminator is blindly guessing what is real vs. synthetic. This is a common starting point, where neither neural network has optimized for its goal.</li><li><strong>Generator loss is becoming negative while the discriminator loss remains at 0.</strong> This means that the generator is producing better and better synthetic data. The discriminator is improving too, but because the synthetic data quality has increased, it is still unable to clearly differentiate real vs. synthetic data.</li><li><strong>Generator loss has stabilized at a negative value while the discriminator loss remains at 0. </strong>This means that the generator has optimized, creating synthetic data that looks so real, the discriminator cannot tell it apart.</li></ol><p>It is encouraging to see that the general pattern for the <code>RacketSports</code> dataset is similar to a variety of other datasets. These are shown below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/12/Multi-Datsets.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"645\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/12/Multi-Datsets.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/12/Multi-Datsets.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/12/Multi-Datsets.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/12/Multi-Datsets.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>The generator and discriminator loss values for a variety of other datasets all follow the same learning pattern. The dataset names are shown in <strong>bold.</strong> They can be downloaded from the SDV demo module.</em></figcaption></figure><p>Of course, other patterns may be possible for different datasets. But if loss values are not stabilizing, watch out! This would indicate that the neural networks were not able to effectively learn patterns in the real data.</p><h3 id=\"metrics-powered-analysis\">Metrics-Powered Analysis</h3><p>You may be wondering whether to trust the loss values. Do they indicate a meaningful difference in synthetic data quality? To answer this question, it's helpful to create synthetic data sets after training the model for different numbers of epochs, and assess the quality of the data sets.</p><pre><code class=\"language-python\">NUM_SYNTHETIC_ROWS = len(real_data)\n\nsynthetic_data = model.sample(num_rows=NUM_SYNTHETIC_ROWS)</code></pre><p>It is important to select a few key metrics for a quantifiable quality measure. For our experiments, we chose 4 metrics from the open source <a href=\"https://docs.sdv.dev/sdmetrics/\">SDMetrics library</a>:</p><ul><li><a href=\"https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/kscomplement\"><strong>KSComplement</strong></a> evaluates the shape of numerical columns</li><li><a href=\"https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/tvcomplement\"><strong>TVComplement</strong></a> evaluates the shape of discrete columns</li><li><a href=\"https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/correlationsimilarity\"><strong>CorrelationSimilarity</strong></a> evaluates pairwise correlations between columns</li><li><a href=\"https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/categorycoverage\"><strong>CategoryCoverage</strong></a> evaluates whether the synthetic data covers all possible values</li></ul><p>Each metric produces a score ranging from 0 (worst quality) to 1 (best quality). In the example below, we use the <code>KSComplement</code> metric on a numerical column in the <code>RacketSports</code> dataset.</p><pre><code class=\"language-python\">from sdmetrics.single_column import KSComplement\n\nNUMERICAL_COLUMN_NAME='dim_2'\n\nscore = KSComplement.compute(\n   real_data[NUMERICAL_COLUMN_NAME],\n   synthetic_data[NUMERICAL_COLUMN_NAME])</code></pre><p>Our results validate that the scores do, indeed, correlate with the loss value from the generator: The quality improves as the loss is minimized. Some of the metrics – such as <code>CorrelationSimilarity</code> and <code>CategoricalCoverage</code> – are high to begin with, so there is not much room to improve. But other metrics, like <code>KSComplement</code>, show significant improvement. This is shown in the graph below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/12/CTGAN-Loss-vs.-KSComplement.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1290\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/12/CTGAN-Loss-vs.-KSComplement.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/12/CTGAN-Loss-vs.-KSComplement.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/12/CTGAN-Loss-vs.-KSComplement.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/12/CTGAN-Loss-vs.-KSComplement.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>A comparison of loss values and the KSComplement metric. The two are linked: Lower generator loss (blue) correspond to higher quality scores (green).</em></figcaption></figure><p>It's also possible to visualize the synthetic data that corresponds to a specific metric. For example, <code>KSComplement</code> compares the overall shape of a real and a synthetic data column, so we can visualize it using histograms.</p><pre><code class=\"language-python\">from sdmetrics.reports import utils\n\nutils.get_column_plot(\n  real_data,\n  synthetic_data,\n  column_name=NUMERICAL_COLUMN_NAME,\n  metadata=table_metadata)</code></pre><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/12/CTGAN-Epochs-vs.-Improvement.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"645\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/12/CTGAN-Epochs-vs.-Improvement.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/12/CTGAN-Epochs-vs.-Improvement.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/12/CTGAN-Epochs-vs.-Improvement.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/12/CTGAN-Epochs-vs.-Improvement.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Three histograms were created after training CTGAN for 10, 100 and 500 epochs on the RacketSports dataset. We plotted the dim_2 column. The real data (gray) doesn't change, but the synthetic data (green) improves with more epochs. The KSComplement metric measures the similarity: 0.74, 0.89 and 0.91 (left to right).</em></figcaption></figure><p>Overall, we can conclude that the generator and discriminator losses correspond to the quality metrics that we measured – which means we can trust the loss values, as well as the synthetic data that our CTGAN created!</p><h3 id=\"conclusion\">Conclusion</h3><p>In this article, we explored the improvements that the CTGAN model makes as it iterates over many epochs. We started by interpreting the loss values that each of the neural networks – the generator and the discriminator – reports over time. This helped us reason about how they were progressing. But to fully trust the progress of our model, we then turned to the <a href=\"https://docs.sdv.dev/sdmetrics/\">SDMetrics library</a>, which provides metrics that are easier to interpret. Using this library, we could verify whether the reported loss values truly resulted in synthetic data quality improvements.</p><p>This may lead us to a new, potential feature: What if we integrated these easily interpretable, user-centric metrics into the CTGAN training progress? This feature would allow you to specify the exact metrics you'd like to optimize upfront – for example, KSComplement. In addition to the generator and discriminator loss, CTGAN may be able to report a snapshot of this metric. A hypothetical example is shown below.</p><pre><code class=\"language-python\">model = CTGAN(\n  table_metadata,\n  verbose=True,\n  epochs=800,\n  optimization_metric='KSComplement',\n  optimization_column='dim_2')\n  \nmodel.fit(real_data)</code></pre><pre><code>Epoch 1, Loss G: 1.0435, Loss D: -0.1401, KSComplement: 0.7832\nEpoch 2, Loss G: 0.4489, Loss D: -0.1455, KSComplement: 0.7671\nEpoch 3, Loss G: 0.4756, Loss D: -0.0956, KSComplement: 0.7664\n…\nEpoch 200: Loss G: -2.542, Loss D: 0.0002911, KSComplement: 0.92391\n</code></pre><p>Such a feature would allow more transparency over CTGAN's learning process, and allow you to stop training your models once the metrics are high. </p><p><strong>What do you think? </strong>If you're interested in exploring the inner workings of CTGAN and optimizing your synthetic data, drop us a comment below!</p>","url":"https://sdv.ghost.io/interpreting-ctgan-progress/","canonical_url":"https://datacebo.com/blog/interpreting-ctg-progress","uuid":"b5ed5d15-21b9-4da5-8f28-7682e7522844","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"63a0bc44ac52ed003d6a169a","reading_time":7}},{"node":{"id":"Ghost__Post__633b5bbbda16fc003d4eab71","title":"How to evaluate synthetic data for your project — and avoid the biggest mistake we see","slug":"how-to-evaluate-synthetic-data","featured":true,"feature_image":"https://sdv.ghost.io/content/images/2022/10/Header-V2.png","excerpt":"Proper evaluation is critical when using synthetic data. Avoid this common mistake and lead your project to success.","custom_excerpt":"Proper evaluation is critical when using synthetic data. Avoid this common mistake and lead your project to success.","visibility":"public","created_at_pretty":"03 October, 2022","published_at_pretty":"07 October, 2022","updated_at_pretty":"10 January, 2023","created_at":"2022-10-03T18:01:31.000-04:00","published_at":"2022-10-07T10:24:15.000-04:00","updated_at":"2023-01-10T13:00:46.000-05:00","meta_title":"How to evaluate synthetic data for your project","meta_description":"Proper evaluation is critical when using synthetic data. Avoid this common mistake and lead your project to success.","og_description":null,"og_image":null,"og_title":null,"twitter_description":"Proper evaluation is critical when using synthetic data. Avoid this common mistake and lead your project to success.","twitter_image":null,"twitter_title":"How to evaluate synthetic data for your project","authors":[{"name":"Neha Patki","slug":"neha","bio":"Neha first created the SDV for her Master's thesis at MIT and also has experience in Product Management from Google. She is excited to use her expertise to build a great user experience at DataCebo.","profile_image":"https://sdv.ghost.io/content/images/2021/05/Neha_Patki--1-.jpg","twitter":"@n4atki","facebook":null,"website":"https://www.linkedin.com/in/nehapatki/"}],"primary_author":{"name":"Neha Patki","slug":"neha","bio":"Neha first created the SDV for her Master's thesis at MIT and also has experience in Product Management from Google. She is excited to use her expertise to build a great user experience at DataCebo.","profile_image":"https://sdv.ghost.io/content/images/2021/05/Neha_Patki--1-.jpg","twitter":"@n4atki","facebook":null,"website":"https://www.linkedin.com/in/nehapatki/"},"primary_tag":{"name":"Product","slug":"product","description":"Our product blog provides insights and tips into creating and deploying high quality synthetic data. We'll support our findings with experiments, illustrations and tutorials.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Product","slug":"product","description":"Our product blog provides insights and tips into creating and deploying high quality synthetic data. We'll support our findings with experiments, illustrations and tutorials.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"In recent years, synthetic data has shown great promise for solving a variety of\nproblems – like addressing data scarcity for AI\n[https://www.gartner.com/en/newsroom/press-releases/2022-06-22-is-synthetic-data-the-future-of-ai] \nand overcoming barriers to data access\n[https://www.agmatix.com/blog/driving-innovation-in-agriculture-with-synthetic-data/?utm_source=LinkedIn&utm_medium=Social]\n. As your organization becomes serious about adopting synthetic data, it's\ncrucial to incorporate the right metrics and evaluation frameworks into your\nprojects.\n\nSince the synthetic data space is so new, there aren't yet industry standards\nfor setting and measuring outcomes. At DataCebo [https://datacebo.com/], we've\nworked with a variety of teams using synthetic data. In this article, we're\nsharing the best practices we've learned along the way, as well as the one key\nmistake to avoid.\n\nWhat are synthetic data metrics?\nIn some fields – such as synthetic image generation – it's easy to visually\ninspect the output (in this case, synthetic images) and determine its quality.\nBut if you are creating synthetic data in a tabular format (with rows and\ncolumns), it's difficult to make an overall assessment just by looking at the\nraw data. This is evident in the table below:\n\nWhen the synthetic data is tabular, it's hard to assess its quality. In this\nexample, 3 of the rows show data from real students, while the other 3 are\nsynthetically created. Can you tell which is which?For tabular synthetic data,\nit's necessary to create metrics that quantify how the synthetic data compares\nto the real data. Each metric measures a particular aspect of the data – such as\ncoverage or correlation – allowing you to identify which specific elements have\nbeen preserved or forgotten during the synthetic data process.\n\nIn our open source library, SDMetrics [https://github.com/sdv-dev/SDMetrics],\nwe've provided a variety of metrics for evaluating synthetic data against the\nreal data. For instance, you can use the CategoryCoverage\n[https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/categorycoverage] and \nRangeCoverage\n[https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/rangecoverage] metrics\nto quantify whether your synthetic data covers the same range of possible values\nas the real data:\n\nIn this example, the numerical distributions of real and synthetic data are\noverlaid to compare coverage. Using SDMetrics, you can apply the RangeCoverage,\nwhich quantifies the coverage. In this case: 82%.You may also be curious about\nwhether the synthetic data captures trends between pairs of columns. To compare\ncorrelations, you can use the CorrelationSimilarity\n[https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/correlationsimilarity] \nmetric:\n\nThis example shows two side-by-side heatmaps of the pairwise correlations for\nreal and synthetic data. Using SDMetrics, you can apply the\nCorrelationSimilarity metric, which quantifies the similarity as a score from 0\nto 1.The SDMetrics library has over 30 metrics, with more still in development.\n\nBut having access to metrics is just one part of the story. With so many\nmetrics, it can be difficult to decide which ones to focus on – and how to make\nprogress in your synthetic data project. To successfully deploy synthetic data,\nit's important to consider metrics during all steps of your project development\ncycle.\n\nIn the rest of this article, we'll share a 3-step plan for incorporating metrics\n– and the SDMetrics library – into your synthetic data project to increase your\nchances of success. \n\nStep 1: Start with the project goals \nIt is tempting to create synthetic data quickly and then test it using all the\navailable metrics. After all, it's hard not to be curious about what synthetic\ndata can do! But to succeed with your project, it's important to take a step\nback and focus on the problem you are trying to solve first.\n\nSynthetic data creation isn't an end in itself. Just as with most data work, you\ndon't create synthetic data for its own sake — you use it to solve a problem. If\nyou want your synthetic data project to succeed, pay close attention to what\nthat problem is, as this will help you narrow down a few key metrics. \n\nFor example, imagine that your organization has two different synthetic data\nprojects related to software testing and machine learning, respectively. Because\nthese projects have different goals, you’ll need to consider different metrics:\n\nUse your project goals to help you decide which metrics to prioritize.Focusing\non your goals allows you to identify which metrics are important for the\nultimate success of your project. The single biggest mistake we see people\nmaking is to skip this critical step. Without focus, you can easily get bogged\ndown running multiple tests and tweaking your synthetic dataset, rather than\nmeeting the specific considerations for your project. This can derail your\nefforts – leading to a complex project that doesn't add any value.\n\nStep 2: Let your goals guide the synthetic data creation \nYour goals can help you appropriately scope your project and cut costs. A core\nsubset of metrics can guide your synthetic data creation, making it faster and\nmore targeted to your needs.\n\nChances are, you'll be faced with many decisions throughout your project. For\nexample, in the SDV library [https://github.com/sdv-dev/SDV], there are 5\ndifferent algorithms that create synthetic data, each with their own settings\nthat lead to hundreds of potential models you can create. But if you know that\nyour synthetic data project is software testing, you've identified that\ncoverage, boundaries and business rules are the highest priority metrics. This\nwill guide your decision-making.\n\nIn this case, you may find success choosing our preset model, FAST ML\n[https://github.com/sdv-dev/SDV/discussions/786]. This model uses statistical\nmethods to achieve your minimal requirements while also providing high\nperformance – FAST ML can train a mid-size data table (100 columns and 100K\nrows) in only a few minutes. You can compare this to other GAN-based models that\nare more resource-intensive, taking hours to finish. If your project metrics are\nsatisfied with FAST ML, it is reasonable to choose this model over a GAN, even\nif it isn't perfectly optimized across all possible metrics.\n\nFrom this example, we can see that metrics are not just something to evaluate at\nthe end of the project – they are useful tools for decision-making throughout \nyour project. \n\nStep 3: Test the end-to-end workflow upfront\nThe purpose of metrics is to provide guardrails and focus for your project –\nscoping it so that you can drive business value most efficiently. For the\nhighest chances of success, it's important to apply the synthetic data\nend-to-end for downstream applications, so that you can verify that business\nvalue upfront.\n\nContinuing with our example of software testing, it's important to use the\nsynthetic data for your downstream software testing suite as quickly as you can\nto verify the benefits of synthetic data. If you've chosen your metrics\ncorrectly and considered them when making decisions (steps #1 and #2), then\nyou'll see that this translates to business value.\n\nGoing end-to-end means applying the synthetic data and verifying the ultimate\nbusiness value it provides.This type of cost-benefit analysis can help you make\nthe case for synthetic data adoption within your enterprise. It can also help\nyou iterate – to get more business value from your synthetic data, you can\ncontinue to optimize the metrics you've chosen in step #1, or identify new ones.\n\nThe Takeaway\nTechnically, there are an infinite number of metrics that could be used to\nevaluate synthetic data. The key to success is to incorporate select metrics\ninto your synthetic data project development rather than just applying all\nmetrics at the end.\n\nYour project goals are critical to helping you choose the right metrics. Setting\nthem upfront allows you to make better decisions during your project\ndevelopment. And going end-to-end allows you to measure the business value that\nyour synthetic data brings to the organization.\n\nWhat are your thoughts? Leave comments below! If you noticed other evaluation\npitfalls in your projects, let us know below or reach us directly at \ninfo@sdv.dev.","html":"<p>In recent years, synthetic data has shown great promise for solving a variety of problems – like <a href=\"https://www.gartner.com/en/newsroom/press-releases/2022-06-22-is-synthetic-data-the-future-of-ai\">addressing data scarcity for AI</a> and <a href=\"https://www.agmatix.com/blog/driving-innovation-in-agriculture-with-synthetic-data/?utm_source=LinkedIn&amp;utm_medium=Social\">overcoming barriers to data access</a>. As your organization becomes serious about adopting synthetic data, it's crucial to incorporate the right metrics and evaluation frameworks into your projects.</p><p>Since the synthetic data space is so new, there aren't yet industry standards for setting and measuring outcomes. At <a href=\"https://datacebo.com/\">DataCebo</a>, we've worked with a variety of teams using synthetic data. In this article, we're sharing the best practices we've learned along the way, as well as the one key mistake to avoid.</p><h3 id=\"what-are-synthetic-data-metrics\">What are synthetic data metrics?</h3><p>In some fields – such as synthetic image generation – it's easy to visually inspect the output (in this case, synthetic images) and determine its quality. But if you are creating synthetic data in a tabular format (with rows and columns), it's difficult to make an overall assessment just by looking at the raw data. This is evident in the table below:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/10/Real-vs.-Synthetic-Data.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"667\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/10/Real-vs.-Synthetic-Data.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/10/Real-vs.-Synthetic-Data.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/10/Real-vs.-Synthetic-Data.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/10/Real-vs.-Synthetic-Data.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>When the synthetic data is tabular, it's hard to assess its quality. In this example, 3 of the rows show data from real students, while the other 3 are synthetically created. Can you tell which is which?</em></figcaption></figure><p>For tabular synthetic data, it's necessary to create metrics that quantify how the synthetic data compares to the real data. Each metric measures a particular aspect of the data – such as coverage or correlation – allowing you to identify which specific elements have been preserved or forgotten during the synthetic data process.</p><p>In our open source library, <a href=\"https://github.com/sdv-dev/SDMetrics\">SDMetrics</a>, we've provided a variety of metrics for evaluating synthetic data against the real data. For instance, you can use the <a href=\"https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/categorycoverage\">CategoryCoverage</a> and <a href=\"https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/rangecoverage\">RangeCoverage</a> metrics to quantify whether your synthetic data covers the same range of possible values as the real data:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/10/Range-Coverage.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"645\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/10/Range-Coverage.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/10/Range-Coverage.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/10/Range-Coverage.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/10/Range-Coverage.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>In this example, the numerical distributions of real and synthetic data are overlaid to compare coverage. Using SDMetrics, you can apply the RangeCoverage, which quantifies the coverage. In this case: 82%.</em></figcaption></figure><p>You may also be curious about whether the synthetic data captures trends between pairs of columns. To compare correlations, you can use the <a href=\"https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/correlationsimilarity\">CorrelationSimilarity</a> metric:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/10/Column-Pairs.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"645\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/10/Column-Pairs.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/10/Column-Pairs.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/10/Column-Pairs.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/10/Column-Pairs.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>This example shows two side-by-side heatmaps of the pairwise correlations for real and synthetic data. Using SDMetrics, you can apply the CorrelationSimilarity metric, which quantifies the similarity as a score from 0 to 1.</em></figcaption></figure><p>The SDMetrics library has over 30 metrics, with more still in development.</p><p>But having access to metrics is just one part of the story. With so many metrics, it can be difficult to decide which ones to focus on – and how to make progress in your synthetic data project. <strong>To successfully deploy synthetic data, it's important to consider metrics during all steps of your project development cycle.</strong></p><p>In the rest of this article, we'll share a 3-step plan for incorporating metrics – and the SDMetrics library – into your synthetic data project to increase your chances of success. </p><h3 id=\"step-1-start-with-the-project-goals\">Step 1: Start with the project goals </h3><p>It is tempting to create synthetic data quickly and then test it using all the available metrics. After all, it's hard not to be curious about what synthetic data can do! But to succeed with your project, it's important to take a step back and focus on the problem you are trying to solve first.</p><p>Synthetic data creation isn't an end in itself. Just as with most data work, you don't create synthetic data for its own sake — you use it to solve a problem. If you want your synthetic data project to succeed, pay close attention to what that problem is, as this will help you narrow down a few key metrics. </p><p>For example, imagine that your organization has two different synthetic data projects related to software testing and machine learning, respectively. Because these projects have different goals, you’ll need to consider different metrics:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/10/Project-Driven-Metrics.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"667\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/10/Project-Driven-Metrics.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/10/Project-Driven-Metrics.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/10/Project-Driven-Metrics.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/10/Project-Driven-Metrics.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Use your project goals to help you decide which metrics to prioritize.</figcaption></figure><p>Focusing on your goals allows you to identify which metrics are important for the ultimate success of your project. <strong>The single biggest mistake we see people making is to skip this critical step.</strong> Without focus, you can easily get bogged down running multiple tests and tweaking your synthetic dataset, rather than meeting the specific considerations for your project. This can derail your efforts – leading to a complex project that doesn't add any value.</p><h3 id=\"step-2-let-your-goals-guide-the-synthetic-data-creation\">Step 2: Let your goals guide the synthetic data creation </h3><p>Your goals can help you appropriately scope your project and cut costs. A core subset of metrics can guide your synthetic data creation, making it faster and more targeted to your needs.</p><p>Chances are, you'll be faced with many decisions throughout your project. For example, in the <a href=\"https://github.com/sdv-dev/SDV\">SDV library</a>, there are 5 different algorithms that create synthetic data, each with their own settings that lead to hundreds of potential models you can create. But if you know that your synthetic data project is software testing, you've identified that coverage, boundaries and business rules are the highest priority metrics. This will guide your decision-making.</p><p>In this case, you may find success choosing our preset model, <a href=\"https://github.com/sdv-dev/SDV/discussions/786\">FAST ML</a>. This model uses statistical methods to achieve your minimal requirements while also providing high performance – FAST ML can train a mid-size data table (100 columns and 100K rows) in only a few minutes. You can compare this to other GAN-based models that are more resource-intensive, taking hours to finish. If your project metrics are satisfied with FAST ML, it is reasonable to choose this model over a GAN, even if it isn't perfectly optimized across all possible metrics.</p><p>From this example, we can see that metrics are not just something to evaluate at the end of the project – they are useful tools for decision-making <em>throughout</em> your project. </p><h3 id=\"step-3-test-the-end-to-end-workflow-upfront\">Step 3: Test the end-to-end workflow upfront</h3><p>The purpose of metrics is to provide guardrails and focus for your project – scoping it so that you can drive business value most efficiently. For the highest chances of success, it's important to apply the synthetic data end-to-end for downstream applications, so that you can verify that business value upfront.</p><p>Continuing with our example of software testing, it's important to use the synthetic data for your downstream software testing suite as quickly as you can to verify the benefits of synthetic data. If you've chosen your metrics correctly and considered them when making decisions (steps #1 and #2), then you'll see that this translates to business value.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/10/Business-Value-Driven-Metrics.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"667\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/10/Business-Value-Driven-Metrics.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/10/Business-Value-Driven-Metrics.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/10/Business-Value-Driven-Metrics.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/10/Business-Value-Driven-Metrics.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Going end-to-end means applying the synthetic data and verifying the ultimate business value it provides.</figcaption></figure><p>This type of cost-benefit analysis can help you make the case for synthetic data adoption within your enterprise. It can also help you iterate – to get more business value from your synthetic data, you can continue to optimize the metrics you've chosen in step #1, or identify new ones.</p><h3 id=\"the-takeaway\">The Takeaway</h3><p>Technically, there are an infinite number of metrics that could be used to evaluate synthetic data. The key to success is to incorporate select metrics into your synthetic data project development rather than just applying all metrics at the end.</p><p>Your project goals are critical to helping you choose the right metrics. Setting them upfront allows you to make better decisions during your project development. And going end-to-end allows you to measure the business value that your synthetic data brings to the organization.</p><p><strong>What are your thoughts? </strong>Leave comments below! If you noticed other evaluation pitfalls in your projects, let us know below or reach us directly at <a href=\"mailto:info@sdv.dev\">info@sdv.dev</a>.</p>","url":"https://sdv.ghost.io/how-to-evaluate-synthetic-data/","canonical_url":null,"uuid":"1cbbc888-463b-4494-8e88-a71353ac697a","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"633b5bbbda16fc003d4eab71","reading_time":6}},{"node":{"id":"Ghost__Post__6216679682795d003d91f6e5","title":"ML Model Development using Synthetic Data Clones","slug":"synthetic-clones-for-ml","featured":true,"feature_image":"https://sdv.ghost.io/content/images/2022/02/ML-Model-Development-Banner-04.png","excerpt":"What happens when you train a machine learning model on synthetic data instead of real data? Let's experiment to find out.","custom_excerpt":"What happens when you train a machine learning model on synthetic data instead of real data? Let's experiment to find out.","visibility":"public","created_at_pretty":"23 February, 2022","published_at_pretty":"24 February, 2022","updated_at_pretty":"23 January, 2023","created_at":"2022-02-23T11:57:58.000-05:00","published_at":"2022-02-24T11:33:56.000-05:00","updated_at":"2023-01-23T16:21:53.000-05:00","meta_title":"Using synthetic data clones for ML","meta_description":"What happens when you train a machine learning model on synthetic data instead of real data? Let's experiment to find out.","og_description":null,"og_image":null,"og_title":null,"twitter_description":"What happens when you train a machine learning model on synthetic data instead of real data? Let's experiment.","twitter_image":null,"twitter_title":"Using synthetic data clones for ML","authors":[{"name":"Arnav Modi","slug":"arnav","bio":"Arnav is a high school student and aspiring data scientist who spent his summer learning about the SDV and how synthetic data may be used to perform ML tasks.","profile_image":"https://sdv.ghost.io/content/images/2022/02/Arnav_Modi--1-.jpg","twitter":null,"facebook":null,"website":null}],"primary_author":{"name":"Arnav Modi","slug":"arnav","bio":"Arnav is a high school student and aspiring data scientist who spent his summer learning about the SDV and how synthetic data may be used to perform ML tasks.","profile_image":"https://sdv.ghost.io/content/images/2022/02/Arnav_Modi--1-.jpg","twitter":null,"facebook":null,"website":null},"primary_tag":{"name":"Product","slug":"product","description":"Our product blog provides insights and tips into creating and deploying high quality synthetic data. We'll support our findings with experiments, illustrations and tutorials.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Product","slug":"product","description":"Our product blog provides insights and tips into creating and deploying high quality synthetic data. We'll support our findings with experiments, illustrations and tutorials.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"This article was researched by Arnav Modi, a community user. Arnav is a high school student and aspiring data scientist who spent his summer learning about the SDV and how synthetic data is used to perform ML tasks.\n\nOne potential use for synthetic data is to replace real data in the development of new machine learning (ML) models. Imagine a scenario where you need to build a predictive ML model – perhaps for a function critical to your business, like predicting customer satisfaction or sales success – with one important consideration: The data is sensitive, so only trusted employees can access it with specific credentials.\n\nAccess to sensitive data may create a barrier for a variety of reasons:\n\n * You might not have ML expertise in your organization, which means you need to use external software or contractors to complete the task. However, you are unable to share the data with them.\n * Your data is available on a secure, cloud-based platform for trusted employees to access remotely. They work on this data using interactive notebooks. Every time they lose their connection – due to WiFi outages, their laptops falling asleep, etc. – they may lose their work or have to reconnect.\n * You have a robust authentication system that your team uses. However, it creates a barrier to entry for rapid, iterative collaboration between members, sharing work and debugging data pipelines. As a result, your collaboration is much slower than it would be if your team could access the data without the need to authenticate.\n\nIn cases like this, synthetic data can be an ideal solution: You can create synthetic data based on the original, sensitive data set, and use it more freely during ML development.\n\nOne key question will determine if this method succeeds: Is the synthetic data actually useful for your ML task? We performed an experiment to find out.\n\nIn the rest of this article, we'll describe our experimental setup and findings. (You can double-check our work in this Colab Notebook.)\n\n\nExperimental Setup\n\nIf an ML model is trained using synthetic data instead of real data, what happens to the model's performance? To answer this question, we identified 3 publicly available datasets (Income, Bank and Airline) that are associated with particular ML prediction tasks. The datasets and tasks are summarized below.\n\nOur experiment compared the performance of an  ML model trained on the original data, vs. one trained on the synthetic data provided by the SDV.\n\n * Control (Original data): How successfully can we complete the ML prediction task if we use the real data? Because some predictions are harder than others, this control helped us identify the overall difficulty of these specific tasks.\n * Experiment (Synthetic data): How successfully can we complete the ML prediction task if we use synthetic data instead? We used the SDV's CopulaGAN to generate synthetic data from the three original datasets.\n\nIn order to develop and test the ML model, we turned to the SDMetrics library — specifically the ML Efficacy metrics, which build an ML model and evaluate its performance. We used the Binary Decision Tree Classifier and Binary Logistic Regression models. The overall experimental setup is illustrated below.\n\nTo obtain reliable findings, we ran 3 iterations and averaged the results.\n\n\nResults\n\nThe graph below shows how well we are able to perform an ML task using the original vs the synthetic data.\n\nDiscussion\n\nThe original data quantifies the general difficulty of the ML task. Looking at these values, we can see that the Income Dataset is the hardest task, as neither of our methods were able to get above 90% accuracy using the original data.\n\nComparing the datasets allows us to quantify the suitability of synthetic data for ML development. Our results show a loss of between 1 and 9% of the original efficacy value for all comparisons, with a median loss of roughly 2.5%.\n\nIt's important to note that the simplifications we've made for this experiment may be resulting in worse accuracy than we would see in real-world use.\n\n * Applying CopulaGAN out-of-the-box to each dataset is simplistic. In a real-world scenario, the model's parameters would likely be explicitly tuned and constraints would be used to improve synthetic data quality.\n * The Decision Tree and Logistic Regression evaluators are relatively simplistic ML classifiers. An ML expert (or ML software) might use more advanced techniques.\n * In our scenario, the 3rd party delivers a fully trained, ready-to-go ML model. Another approach is to ask them to use the synthetic data to deliver an untrained model – so that you can train it yourself on the real dataset. This alternative setup, which should increase the prediction accuracy, will be a topic for a future article.\n\nIn summary, the accuracy loss we observe represents the worst case scenario. In a production environment, higher-quality ML models and more careful tuning of the SDV will likely minimize performance differences between original and synthetic data.\n\n\nTakeaways\n\nIn this article, we quantified the effect of replacing real data with a synthetic data clone for ML development. Our results show a loss of 2.5% accuracy when using synthetic data. Considering these results, we assess that it is reasonable to explore the use of synthetic data for the purpose of ML development.\n\nIn order to maximize the utility of the synthetic data, we recommend tuning the SDV model and using constraints to improve the data quality. In future articles, we'll explore more details about using synthetic data for ML.\n\nAre you using the SDV to solve your ML business needs? Publish your findings on the SDV blog as a guest author! Contact us at info@sdv.dev.\n\n\n\n\n\n","html":"<p><em>This article was researched by Arnav Modi, a community user. Arnav is a high school student and aspiring data scientist who spent his summer learning about the SDV and how synthetic data is used to perform ML tasks.</em></p><p>One potential use for synthetic data is to replace real data in the development of new machine learning (ML) models. Imagine a scenario where you need to build a predictive ML model – perhaps for a function critical to your business, like predicting customer satisfaction or sales success – with one important consideration: <strong>The data is sensitive, so only trusted employees can access it with specific credentials.</strong></p><p>Access to sensitive data may create a barrier for a variety of reasons:</p><ul><li>You might not have ML expertise in your organization, which means you need to use external software or contractors to complete the task. However, you are unable to share the data with them.</li><li>Your data is available on a secure, cloud-based platform for trusted employees to access remotely. They work on this data using interactive notebooks. Every time they lose their connection – due to WiFi outages, their laptops falling asleep, etc. – they may lose their work or have to reconnect.</li><li>You have a robust authentication system that your team uses. However, it creates a barrier to entry for rapid, iterative collaboration between members, sharing work and debugging data pipelines. As a result, your collaboration is much slower than it would be if your team could access the data without the need to authenticate.</li></ul><p>In cases like this, synthetic data can be an ideal solution: You can create synthetic data based on the original, sensitive data set, and use it more freely during ML development.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/02/ML-Model-Development-03.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"783\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/02/ML-Model-Development-03.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/02/ML-Model-Development-03.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/02/ML-Model-Development-03.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/02/ML-Model-Development-03.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Synthetic data can be useful for ML development. You can use synthetic data to develop models in a variety of environments, like data science platforms, local machines or 3rd party software. Meanwhile, the real data never leaves your premises.</figcaption></figure><p>One key question will determine if this method succeeds: Is the synthetic data actually useful for your ML task? We performed an experiment to find out.</p><p>In the rest of this article, we'll describe our experimental setup and findings. (You can double-check our work in this <a href=\"https://colab.research.google.com/drive/13-1xy5t7veizWBsb_dDgTRBdhGcCqjCJ?usp=sharing\">Colab Notebook</a>.)</p><h3 id=\"experimental-setup\">Experimental Setup</h3><p>If an ML model is trained using synthetic data instead of real data, what happens to the model's performance? To answer this question, we identified 3 publicly available datasets (<a href=\"https://www.kaggle.com/mastmustu/income?select=train.csv\">Income</a>, <a href=\"https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\">Bank</a> and <a href=\"https://www.kaggle.com/teejmahal20/airline-passenger-satisfaction?select=train.csv\">Airline</a>) that are associated with particular ML prediction tasks. The datasets and tasks are summarized below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/02/ML-Model-Development-04.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"671\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/02/ML-Model-Development-04.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/02/ML-Model-Development-04.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/02/ML-Model-Development-04.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/02/ML-Model-Development-04.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>A description of our datasets. *[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014</figcaption></figure><p>Our experiment compared the performance of an  ML model trained on the original data, vs. one trained on the synthetic data provided by the SDV.</p><ul><li><strong><strong><strong>Control (Original data): </strong></strong></strong>How successfully can we complete the ML prediction task if we use the real data? Because some predictions are harder than others, this control helped us identify the overall difficulty of these specific tasks.</li><li><strong><strong>Experiment (Synthetic data):</strong> </strong>How successfully can we complete the ML prediction task if we use synthetic data instead? We used the SDV's <a href=\"https://sdv.dev/SDV/user_guides/single_table/copulagan.html\">CopulaGAN</a> to generate synthetic data from the three original datasets.</li></ul><p>In order to develop and test the ML model, we turned to the SDMetrics library — specifically the <a href=\"https://sdv.dev/SDV/user_guides/evaluation/single_table_metrics.html#machine-learning-efficacy-metrics\">ML Efficacy metrics</a>, which build an ML model and evaluate its performance. We used the Binary Decision Tree Classifier and Binary Logistic Regression models. The overall experimental setup is illustrated below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/02/ML-Model-Development-05.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"724\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/02/ML-Model-Development-05.png 600w, https://sdv.ghost.io/content/images/size/w1000/2022/02/ML-Model-Development-05.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2022/02/ML-Model-Development-05.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2022/02/ML-Model-Development-05.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The experimental setup evaluated synthetic data against a test set of original data that we set aside at the start. This allows us to compare the usefulness of both types of data for ML tasks.</figcaption></figure><p>To obtain reliable findings, we ran 3 iterations and averaged the results.</p><h3 id=\"results\">Results</h3><p>The graph below shows how well we are able to perform an ML task using the original vs the synthetic data.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2022/02/Machine-Learning-Efficacy.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"638\" height=\"395\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2022/02/Machine-Learning-Efficacy.png 600w, https://sdv.ghost.io/content/images/2022/02/Machine-Learning-Efficacy.png 638w\"><figcaption>A comparison of ML accuracy scores obtained using real vs. synthetic data, allowing us to assess any loss of accuracy that comes from replacing the original data with synthetic data.</figcaption></figure><p><strong>Discussion</strong></p><p>The original data quantifies the general difficulty of the ML task. Looking at these values, we can see that the Income Dataset is the hardest task, as neither of our methods were able to get above 90% accuracy using the original data.</p><p>Comparing the datasets allows us to quantify the suitability of synthetic data for ML development. Our results show a loss of between 1 and 9% of the original efficacy value for all comparisons, with a median loss of roughly 2.5%.</p><p>It's important to note that the simplifications we've made for this experiment may be resulting in worse accuracy than we would see in real-world use.</p><ul><li>Applying CopulaGAN out-of-the-box to each dataset is simplistic. In a real-world scenario, the model's parameters would likely be explicitly <a href=\"https://sdv.dev/SDV/user_guides/single_table/copulagan.html\">tuned</a> and <a href=\"https://sdv.dev/SDV/user_guides/single_table/constraints.html\">constraints</a> would be used to improve synthetic data quality.</li><li>The Decision Tree and Logistic Regression evaluators are relatively simplistic ML classifiers. An ML expert (or ML software) might use more advanced techniques.</li><li>In our scenario, the 3rd party delivers a fully trained, ready-to-go ML model. Another approach is to ask them to use the synthetic data to deliver an <em>untrained</em> model – so that you can train it yourself on the real dataset. This alternative setup, which should increase the prediction accuracy, will be a topic for a future article.</li></ul><p>In summary, the accuracy loss we observe represents the worst case scenario. In a production environment, higher-quality ML models and more careful tuning of the SDV will likely minimize performance differences between original and synthetic data.</p><h3 id=\"takeaways\">Takeaways</h3><p>In this article, we quantified the effect of replacing real data with a synthetic data clone for ML development. Our results show a loss of 2.5% accuracy when using synthetic data. Considering these results, we assess that <strong>it is reasonable to explore the use of synthetic data for the purpose of ML development</strong>.</p><p>In order to maximize the utility of the synthetic data, we recommend tuning the SDV model and using constraints to improve the data quality. In future articles, we'll explore more details about using synthetic data for ML.</p><p><em>Are you using the SDV to solve your ML business needs? Publish your findings on the SDV blog as a guest author! Contact us at </em><a href=\"mailto:info@sdv.dev\"><em>info@sdv.dev</em></a><em>.</em></p><p><br></p><p><br></p>","url":"https://sdv.ghost.io/synthetic-clones-for-ml/","canonical_url":"https://datacebo.com/blog/synthetic-clones-for-ml","uuid":"252b7f1d-c113-4a0f-a712-3e5342d2e39a","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"6216679682795d003d91f6e5","reading_time":5}},{"node":{"id":"Ghost__Post__61a68d091b683e0048b2a2f3","title":"User input to enhance synthetic data generation","slug":"user-input-synthetic-data","featured":false,"feature_image":"https://sdv.ghost.io/content/images/2021/11/Banner.png","excerpt":"ML models learn some rules out of the box, while other logic requires more work. Which is which? Read more to find out.","custom_excerpt":"ML models learn some rules out of the box, while other logic requires more work. Which is which? Read more to find out.","visibility":"public","created_at_pretty":"30 November, 2021","published_at_pretty":"01 December, 2021","updated_at_pretty":"02 December, 2021","created_at":"2021-11-30T15:43:53.000-05:00","published_at":"2021-12-01T11:06:49.000-05:00","updated_at":"2021-12-02T18:23:08.000-05:00","meta_title":"User input to enhance synthetic data generation","meta_description":"ML models learn some rules out of the box, while other logic requires more work. Which is which? Read more to find out.","og_description":null,"og_image":null,"og_title":null,"twitter_description":"ML models learn some rules out of the box, while other logic requires more work. Which is which? Read more to find out.","twitter_image":"https://sdv.ghost.io/content/images/2021/11/Banner-1.png","twitter_title":"User input to enhance synthetic data generation","authors":[{"name":"Neha Patki","slug":"neha","bio":"Neha first created the SDV for her Master's thesis at MIT and also has experience in Product Management from Google. She is excited to use her expertise to build a great user experience at DataCebo.","profile_image":"https://sdv.ghost.io/content/images/2021/05/Neha_Patki--1-.jpg","twitter":"@n4atki","facebook":null,"website":"https://www.linkedin.com/in/nehapatki/"}],"primary_author":{"name":"Neha Patki","slug":"neha","bio":"Neha first created the SDV for her Master's thesis at MIT and also has experience in Product Management from Google. She is excited to use her expertise to build a great user experience at DataCebo.","profile_image":"https://sdv.ghost.io/content/images/2021/05/Neha_Patki--1-.jpg","twitter":"@n4atki","facebook":null,"website":"https://www.linkedin.com/in/nehapatki/"},"primary_tag":{"name":"Product","slug":"product","description":"Our product blog provides insights and tips into creating and deploying high quality synthetic data. We'll support our findings with experiments, illustrations and tutorials.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Product","slug":"product","description":"Our product blog provides insights and tips into creating and deploying high quality synthetic data. We'll support our findings with experiments, illustrations and tutorials.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"In our previous article [https://sdv.dev/blog/fake-to-synthetic-ml], we explored\nhow machine learning (ML) plays a key role in synthetic data creation. One of\nthe biggest strengths of ML is automatic rule detection (also known in ML terms\nas correlations): The algorithms are designed to learn patterns in the data,\neven without additional user input. The result is synthetic data that resembles\nthe original, right down to its mathematical properties!\n\nHowever, in some cases, applying an ML model right out of the box may not\nimmediately achieve the desired result. In this article, we'll explore the\nstrengths of ML models and go through those areas where user input may be\nrequired.\n\nStrengths of ML Models\nThe goal of any ML-based synthetic data generation software is to learn from and\nemulate the input data. To illustrate this, let's pretend you work in the car\ninsurance business, and you're in possession of a real dataset related to\ndrivers and their insurance:\n\nAn example dataset, including license and collision coverage information\nassociated with different drivers.An ML-based system, such as the Synthetic Data Vault\n[https://sdv.dev/blog/intro-to-sdv/] (SDV), will learn patterns from the real\ndata and use it to create new synthetic data. Recall some of the important\npatterns that ML algorithms detect:\n\n * Shapes. The general shape of the data. For example, in the dataset above, 50%\n   of drivers have Collision Coverage and the Annual Premium is uniformly\n   scattered between $3,000 and $9,000.\n * Correlations. The trends within the data. For example, having Collision\n   Coverage -- especially Standard coverage -- means a higher Annual Premium.\n\nThese shapes and correlations will be present in the synthetic data that is\noutputted by the ML model, as shown below.\n\nAn example of a synthetic dataset created by an ML-based algorithm. The\nalgorithm will learn patterns from the real data and emulate them.Perhaps the single biggest strength of an ML algorithm is its ability to learn\nrules by looking for general patterns in the data, using probability and\nstatistics.\n\nWhat ML models do not learn out of the box\nLet's take a closer look at the synthetic car insurance data. You might notice\nthat two of the rows in the synthetic data don't make complete sense. Below,\nwe've highlighted the errors.\n\nThe synthetic car insurance data, with errors highlighted.Do you see what has\ngone wrong? In the first row, the license expired 3 years earlier than it was\nissued. In the last row, a driver without Collision Coverage has a Collision\nPolicy Type. Additionally, the same Customer ID has been repeated in Row 3 and\nRow 4.\n\nThere are three rules that the ML algorithm did not follow:\n\n 1. License Expiration > License Issue Year\n 2. If Has Collision Coverage = NO, then Collision Policy Type must be empty\n 3. All Customer IDs must be unique\n\nWhy does the ML model easily pick up on some rules and not others? To answer\nthis question, we can look closely at the rules themselves. All of the rules\nthat the ML model successfully learned -- including the distribution shapes and\nthe correlations -- were based on general trends. These probabilistic rules \napply to a majority of the relationships within the dataset, but not all of\nthem. Although they have to make sense in aggregate, a few rows may be\nexceptions.\n\nBy contrast, the rules that the ML model failed to learn were stricter. These \ndeterministic rules describe intrinsic laws of nature, time or logic. Each and\nevery row must adhere to them, and they won't change regardless of  how much (or\nhow little) data has been given to the ML model.\n\nTo continue with the driving theme: A probabilistic rule is like a yield sign,\nsignaling a general recommendation that works out differently for each\nindividual situation -- some cars may need to stop, while others just slow down.\nMeanwhile, a deterministic rule is like a stop sign, demanding that every single\ncar must come to a full stop.\n\nA probabilistic rule applies to a majority of rows, but leaves room for\nexceptions. Meanwhile, a deterministic rule applies to every single row.By\ndefault, our ML model assumed that all rules were probabilistic. When this\nhappens, synthetic data still generally follows the desired properties -- for\nexample, License Expiration > License Issue Year -- for most of the rows, but\nnot for every row.\n\nImproving the ML Models using constraints\nJust because the ML model didn't automatically follow a deterministic rule\ndoesn't mean that it can't. It's possible to improve the model so that it\nunderstands this type of rule. As a user working with the SDV, you can input\ndeterministic rules into your model using constraints.\n\nAn ML model built using constraints will accommodate both probabilistic and\ndeterministic rules.\n\nDo you need SDV constraints?\n\nDeterministic rules are often easy to spot in your dataset: They are the rules\nthat every single row must follow in order to be valid, regardless of how much\ndata there is overall.  But even if you identify the right constraints, there\nare some cases where you might not actually want to supply them to the SDV.\n\nBecause the SDV learns probabilistic rules, most of the synthesized data is\ngenerally valid. Having a few errors sprinkled in might actually be beneficial\nif you want your synthetic data to cover some edge cases. For example, if you're\nusing the synthetic data to test insurance claim software, leaving in some weird\ndata points might help you ensure that the software can handle unexpected cases\n-- like the License Expiration accidentally being set too early.\n\nThe figure below shows a few questions you can ask to determine whether adding a\nconstraint is the right approach.\n\nShould you input a rule using constraints? First, determine whether the rule is\ndeterministic, and then take your use case into account.The SDV Constraints\noffering\n\nIf you decide that adding deterministic rules is important for generating your\nsynthetic data, the SDV has many different constraints to choose from! The table\nbelow describes the constraints you would need in order to define the\ndeterministic rules that would best mold your Car Insurance dataset.\n\nThe GreaterThan, ColumnFormula and Unique constraints -- all available in the\nSDV -- set the deterministic rules that ensure your synthetic Car Insurance Data\nis useful and makes sense.The SDV offers many more possible constraints,\nincluding:\n\n * UniqueCombinations\n * Positive and Negative\n * Rounding\n * Between\n * OneHotEncoding\n\nYou can add multiple constraints to the same dataset in order to accommodate all\nthe deterministic rules you need. For more details, read the Constraints User\nGuide [https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html].\n\nTakeaways\nIn this article, we learned that:\n\n * Data is governed by rules. The SDV automatically learns probabilistic rules,\n   which describe overall trends or patterns in the data.\n * However, sometimes the data has deterministic rules, which are always\n   inherent no matter how much or how little data there is. ML-based systems,\n   including the SDV, may not enforce deterministic rules out of the box.\n * Users can input deterministic rules to the SDV using constraints. To figure\n   out whether you should input a constraint, ask yourself whether there are any\n   rules that the data must always follow. There are many constraints to choose\n   from.\n\nIn future articles, we'll dive deeper into this topic. We'll explore the\ntechnical details behind constraints, and how exactly the SDV's ML models are\nable to learn deterministic rules.","html":"<p>In our <a href=\"https://sdv.dev/blog/fake-to-synthetic-ml\">previous article</a>, we explored how machine learning (ML) plays a key role in synthetic data creation. One of the biggest strengths of ML is <em>automatic rule detection</em> (also known in ML terms as <em>correlations</em>): The algorithms are designed to learn patterns in the data, even without additional user input. The result is synthetic data that resembles the original, right down to its mathematical properties!</p><p>However, in some cases, applying an ML model right out of the box may not immediately achieve the desired result. In this article, we'll explore the strengths of ML models and go through those areas where user input may be required.</p><h3 id=\"strengths-of-ml-models\">Strengths of ML Models</h3><p>The goal of any ML-based synthetic data generation software is to learn from and emulate the input data. To illustrate this, let's pretend you work in the car insurance business, and you're in possession of a real dataset related to drivers and their insurance:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2021/11/Figure-03.png\" class=\"kg-image\" alt=\"An example dataset, including license and collision coverage information associated with different drivers.\" loading=\"lazy\" width=\"1916\" height=\"835\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/11/Figure-03.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/11/Figure-03.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/11/Figure-03.png 1600w, https://sdv.ghost.io/content/images/2021/11/Figure-03.png 1916w\" sizes=\"(min-width: 720px) 720px\"><figcaption>An example dataset, including license and collision coverage information associated with different drivers.</figcaption></figure><p>An ML-based system, such as the <a href=\"https://sdv.dev/blog/intro-to-sdv/\">Synthetic Data Vault</a> (SDV), will learn patterns from the real data and use it to create new synthetic data. Recall some of the important patterns that ML algorithms detect:</p><ul><li><strong><strong><strong>Shapes. </strong></strong></strong>The general shape of the data. For example, in the dataset above, 50% of drivers have Collision Coverage and the Annual Premium is uniformly scattered between $3,000 and $9,000.</li><li><strong><strong><strong>Correlations.</strong> </strong></strong>The trends within the data. For example, having Collision Coverage -- especially Standard coverage -- means a higher Annual Premium.</li></ul><p>These shapes and correlations will be present in the synthetic data that is outputted by the ML model, as shown below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2021/11/Figure-04.png\" class=\"kg-image\" alt=\"An example of a synthetic dataset created by an ML-based algorithm. The algorithm will learn patterns from the real data and emulate them.\" loading=\"lazy\" width=\"1875\" height=\"832\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/11/Figure-04.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/11/Figure-04.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/11/Figure-04.png 1600w, https://sdv.ghost.io/content/images/2021/11/Figure-04.png 1875w\" sizes=\"(min-width: 720px) 720px\"><figcaption>An example of a synthetic dataset created by an ML-based algorithm. The algorithm will learn patterns from the real data and emulate them.</figcaption></figure><p>Perhaps <strong>the single biggest strength of an ML algorithm is its ability to learn rules by looking for general patterns in the data,</strong> using probability and statistics.</p><h3 id=\"what-ml-models-do-not-learn-out-of-the-box\">What ML models do not learn out of the box</h3><p>Let's take a closer look at the synthetic car insurance data. You might notice that two of the rows in the synthetic data don't make complete sense. Below, we've highlighted the errors.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2021/11/Figure-05.png\" class=\"kg-image\" alt=\"The synthetic car insurance data, with errors highlighted. \" loading=\"lazy\" width=\"1867\" height=\"831\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/11/Figure-05.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/11/Figure-05.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/11/Figure-05.png 1600w, https://sdv.ghost.io/content/images/2021/11/Figure-05.png 1867w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The synthetic car insurance data, with errors highlighted.</figcaption></figure><p>Do you see what has gone wrong? In the first row, the license expired 3 years earlier than it was issued. In the last row, a driver without Collision Coverage has a Collision Policy Type. Additionally, the same Customer ID has been repeated in Row 3 and Row 4.</p><p>There are three rules that the ML algorithm did not follow:</p><ol><li>License Expiration &gt; License Issue Year</li><li>If Has Collision Coverage = NO, then Collision Policy Type must be empty</li><li>All Customer IDs must be unique</li></ol><p>Why does the ML model easily pick up on some rules and not others? To answer this question, we can look closely at the rules themselves. All of the rules that the ML model successfully learned -- including the distribution shapes and the correlations -- were based on general trends. These <strong>probabilistic rules</strong> apply to a majority of the relationships within the dataset, but not all of them. Although they have to make sense in aggregate, a few rows may be exceptions.</p><p>By contrast, the rules that the ML model failed to learn were stricter. These <strong>deterministic rules</strong> describe intrinsic laws of nature, time or logic. Each and every row must adhere to them, and they won't change regardless of  how much (or how little) data has been given to the ML model.</p><p>To continue with the driving theme: A probabilistic rule is like a yield sign, signaling a general recommendation that works out differently for each individual situation -- some cars may need to stop, while others just slow down. Meanwhile, a deterministic rule is like a stop sign, demanding that every single car must come to a full stop.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2021/11/Figure-06.png\" class=\"kg-image\" alt=\"A probabilistic rule applies to a majority of rows, but leaves room for exceptions. Meanwhile, a deterministic rule applies to every single row.\" loading=\"lazy\" width=\"1607\" height=\"662\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/11/Figure-06.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/11/Figure-06.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/11/Figure-06.png 1600w, https://sdv.ghost.io/content/images/2021/11/Figure-06.png 1607w\" sizes=\"(min-width: 720px) 720px\"><figcaption>A probabilistic rule applies to a majority of rows, but leaves room for exceptions. Meanwhile, a deterministic rule applies to every single row.</figcaption></figure><p><strong>By default, our ML model assumed that all rules were probabilistic.</strong> When this happens, synthetic data still generally follows the desired properties -- for example, License Expiration &gt; License Issue Year -- for <em>most</em> of the rows, but not for every row.</p><h3 id=\"improving-the-ml-models-using-constraints\">Improving the ML Models using constraints</h3><p>Just because the ML model didn't automatically follow a deterministic rule doesn't mean that it can't. It's possible to improve the model so that it understands this type of rule. As a user working with the SDV, you can input deterministic rules into your model using <strong>constraints</strong>.</p><p>An ML model built using constraints will accommodate both probabilistic and deterministic rules.</p><p><strong>Do you need SDV constraints?</strong></p><p>Deterministic rules are often easy to spot in your dataset: They are the rules that every single row must follow in order to be valid, regardless of how much data there is overall.  But even if you identify the right constraints, there are some cases where you might not actually want to supply them to the SDV.</p><p>Because the SDV learns probabilistic rules, most of the synthesized data is generally valid. Having a few errors sprinkled in might actually be beneficial if you want your synthetic data to cover some edge cases. For example, if you're using the synthetic data to test insurance claim software, leaving in some weird data points might help you ensure that the software can handle unexpected cases -- like the License Expiration accidentally being set too early.</p><p>The figure below shows a few questions you can ask to determine whether adding a constraint is the right approach.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2021/12/Figure-07.png\" class=\"kg-image\" alt=\"Should you input a rule using constraints? First, determine whether the rule is deterministic, and then take your use case into account.\" loading=\"lazy\" width=\"2000\" height=\"586\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/12/Figure-07.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/12/Figure-07.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/12/Figure-07.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2021/12/Figure-07.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Should you input a rule using constraints? First, determine whether the rule is deterministic, and then take your use case into account.</figcaption></figure><p><strong>The SDV Constraints offering</strong></p><p>If you decide that adding deterministic rules is important for generating your synthetic data, the SDV has many different constraints to choose from! The table below describes the constraints you would need in order to define the deterministic rules that would best mold your Car Insurance dataset.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2021/11/Figure-08.png\" class=\"kg-image\" alt=\"The GreaterThan, ColumnFormula and Unique constraints -- all available in the SDV -- set the deterministic rules that ensure your synthetic  Car Insurance Data is useful and makes sense.\" loading=\"lazy\" width=\"2000\" height=\"578\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/11/Figure-08.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/11/Figure-08.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/11/Figure-08.png 1600w, https://sdv.ghost.io/content/images/2021/11/Figure-08.png 2204w\" sizes=\"(min-width: 720px) 720px\"><figcaption>The GreaterThan, ColumnFormula and Unique constraints -- all available in the SDV -- set the deterministic rules that ensure your synthetic Car Insurance Data is useful and makes sense.</figcaption></figure><p>The SDV offers many more possible constraints, including:</p><ul><li>UniqueCombinations</li><li>Positive and Negative</li><li>Rounding</li><li>Between</li><li>OneHotEncoding</li></ul><p>You can add multiple constraints to the same dataset in order to accommodate all the deterministic rules you need. For more details, read the <a href=\"https://sdv.dev/SDV/user_guides/single_table/handling_constraints.html\">Constraints User Guide</a>.</p><h3 id=\"takeaways\">Takeaways</h3><p>In this article, we learned that:</p><ul><li>Data is governed by rules. The SDV automatically learns probabilistic rules, which describe overall trends or patterns in the data.</li><li>However, sometimes the data has <strong>deterministic rules</strong>, which are always inherent no matter how much or how little data there is. ML-based systems, including the SDV, may not enforce deterministic rules out of the box.</li><li>Users can input deterministic rules to the SDV using <strong>constraints</strong>. To figure out whether you should input a constraint, ask yourself whether there are any rules that the data must always follow. There are many constraints to choose from.</li></ul><p>In future articles, we'll dive deeper into this topic. We'll explore the technical details behind constraints, and how exactly the SDV's ML models are able to learn deterministic rules.<br></p>","url":"https://sdv.ghost.io/user-input-synthetic-data/","canonical_url":null,"uuid":"e72409d4-6d39-46dc-a973-3e5f9f039dd7","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"61a68d091b683e0048b2a2f3","reading_time":5}}]}},"pageContext":{"slug":"product","pageNumber":0,"humanPageNumber":1,"skip":0,"limit":12,"numberOfPages":1,"previousPagePath":"","nextPagePath":""}},"staticQueryHashes":["2061773391","2358152166","2362887240","2439066133","2561578252","2657115718","2731221146","2839364760","4145280475"]}