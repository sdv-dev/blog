{"componentChunkName":"component---src-templates-tag-js","path":"/tag/applications/","result":{"data":{"ghostTag":{"slug":"applications","name":"Applications","visibility":"public","feature_image":null,"description":"This blog focuses on the value that synthetic data brings to solving enterprise problems. Synthetic data can be used to test applications, develop unbiased AI models and more. Explore new use cases, concepts and case studies.","meta_title":null,"meta_description":null},"allGhostPost":{"edges":[{"node":{"id":"Ghost__Post__63b4711dac52ed003d6a1744","title":"Can you use synthetic data for label balancing?","slug":"synthetic-label-balancing","featured":true,"feature_image":"https://sdv.ghost.io/content/images/2023/01/Header--1-.png","excerpt":"Imbalanced data can prevent your projects from succeeding. Will synthetic data work? Explore the rationale behind label balancing.","custom_excerpt":"Imbalanced data can prevent your projects from succeeding. Will synthetic data work? Explore the rationale behind label balancing.","visibility":"public","created_at_pretty":"03 January, 2023","published_at_pretty":"10 January, 2023","updated_at_pretty":"23 January, 2023","created_at":"2023-01-03T13:17:01.000-05:00","published_at":"2023-01-10T12:59:16.000-05:00","updated_at":"2023-01-23T14:49:21.000-05:00","meta_title":"Synthetic Data for Label Balancing","meta_description":"Imbalanced data can prevent your projects from succeeding. Will synthetic data work? Explore the rationale behind label balancing.","og_description":null,"og_image":null,"og_title":null,"twitter_description":"Imbalanced data can prevent your projects from succeeding. Will synthetic data work? Explore the rationale behind label balancing.","twitter_image":null,"twitter_title":"Synthetic Data for Label Balancing","authors":[{"name":"Neha Patki","slug":"neha","bio":"Neha first created the SDV for her Master's thesis at MIT and also has experience in Product Management from Google. She is excited to use her expertise to build a great user experience at DataCebo.","profile_image":"https://sdv.ghost.io/content/images/2021/05/Neha_Patki--1-.jpg","twitter":"@n4atki","facebook":null,"website":"https://www.linkedin.com/in/nehapatki/"}],"primary_author":{"name":"Neha Patki","slug":"neha","bio":"Neha first created the SDV for her Master's thesis at MIT and also has experience in Product Management from Google. She is excited to use her expertise to build a great user experience at DataCebo.","profile_image":"https://sdv.ghost.io/content/images/2021/05/Neha_Patki--1-.jpg","twitter":"@n4atki","facebook":null,"website":"https://www.linkedin.com/in/nehapatki/"},"primary_tag":{"name":"Applications","slug":"applications","description":"This blog focuses on the value that synthetic data brings to solving enterprise problems. Synthetic data can be used to test applications, develop unbiased AI models and more. Explore new use cases, concepts and case studies.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Applications","slug":"applications","description":"This blog focuses on the value that synthetic data brings to solving enterprise problems. Synthetic data can be used to test applications, develop unbiased AI models and more. Explore new use cases, concepts and case studies.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"A dataset can unlock many doors for your organization, helping with everything from predictive forecasting to data-driven decision making. But in some situations, you may not have all of the data that you need. A common scenario is having too few data points, which can lead to an imbalance of variables.\n\nIn this article, we'll take a closer look at this scenario. We'll recap why this can be a problem for your projects and walk you through some possible solutions. We'll end by explaining why synthetic data might be especially useful for overcoming this challenge.\n\n\nWhy is it a problem to have a data imbalance?\n\nAt a basic level, data is a record of events, and a data imbalance happens when some events occur much less frequently than others. For example:\n\n * In healthcare, cancer occurs less frequently than diabetes\n * In finance, it's rare to see fraudulent credit card charges\n * In local government, it's rare to have a day with a major natural disaster such as a fire\n\nA natural data imbalance isn't inherently a problem, but it can become one if the rare events are important. As an example, let's assume you're working at a hospital that is treating COVID patients. One day a new COVID variant – let's call it Variant X – appears in the population. Since it is so new, it currently occurs very rarely (<2.5% of the time). This leads to a data imbalance for this COVID variant, as illustrated below.\n\nThis imbalance is a problem when it's critical to account for Variant X. For example, you may want to build a predictive model for who is most likely to be hospitalized. If you use the data as-is, your model may only consider Omicron (the majority) and treat Variant X as an outlier. This can lead to poor predictions – and bad planning – because Variant X may soon become the dominant strain.\n\n\nUsing Data Augmentation to Fix Imbalances\n\nIn an ideal world, your data would include more patients with Variant X. But until then, you need to find a solution that will allow you to produce reasonable predictions. What if you create some artificial patient data for the sake of making a robust predictive model?\n\nLet's assume you have data for 1,000 COVID hospital patients, 975 with the Omicron variant and 25 with Variant X. If you can create 950 additional artificial Variant X patients, then you can create an evenly-balanced dataset. This process is illustrated below.\n\nYou may be skeptical because there are only 25 patients with Variant X to begin with. How can we reasonably produce 950 more based on that? As usual with data science, the devil is in the details. Let's go through some approaches to see what works.\n\n\nAttempt #1: Oversampling\n\nYour first instinct may be to take the existing 25 Variant X patients and weigh them more heavily. One easy way to achieve this: You can duplicate each original patient 40 times to get 1,000 patients.\n\nIn data science, this is known as oversampling. Sometimes, this is done programmatically, sampling patients (with replacement) as many times as needed. Other times, this can be achieved using mathematical formulas to provide weights. An illustration is shown below.\n\nWith oversampling, Variant X is no longer rare, so your model cannot ignore it. But if you actually use this data, your project may not be successful. Your model may confidently predict that all Variant X patients must be over the age of 50. But this is not necessarily right – just because the existing patients had these characteristics doesn't mean Variant X patients always will.\n\nThe mistake was over-emphasizing the same set of patients, making the model more likely to create strong claims. This is commonly referred to as overfitting the data: The model over-emphasizes the importance of a small number of records, and makes blanket predictions that lack nuance.\n\n\nAttempt #2: Randomizing\n\nTo avoid the problems that come with oversampling, let's explore the opposite direction for argument's sake: What if we created artificial Variant X patients by choosing variables completely at random? An example is illustrated below.\n\nRandomization avoids overfitting because no patient is repeated. But this approach introduces problems of its own: You may find that the data doesn't make sense anymore. The example data above highlights some problems that can arise. We see an artificial 23-year-old patient with dementia, and many diabetic patients with low BMIs. In the medical world, these events are not likely and indicate that there is a problem with the data.\n\nThese inconsistencies may (rightfully) dissuade you from using randomization. Since random data lacks patterns, a model will not be able to draw conclusions from it. In data science, we call this a problem of noisiness. Noisy data has too many random combinations to produce any useful learnings.\n\n\nA Better Solution: Defining Neighborhoods\n\nSo far, we've seen attempts at extreme ends.\n\n 1. Oversampling will emphasize one set of patients, leading to an overfit model\n 2. Randomizing will make the dataset noisy, precluding useful conclusions\n\nThe solution we need falls somewhere in the middle: We'd like to loosely base the artificial patients on the real ones. This is related to the data science concept of neighborhoods. Drawing a neighborhood around some patients identifies general commonalities between them – without setting any strict rules. For example, Variant X patients may be more likely to have had a known exposure, but it's not guaranteed. Note that there is no exact definition for a neighborhood. It can change based on our assumptions and how broad we want to make it.\n\nOnce we know a neighborhood, we can create artificial patients that are inside it. These patients won't be exactly the same as the existing ones, but they won't have completely random values either.\n\nSynthetic Data for Label Balancing\n\nA compelling solution for discovering neighborhoods is synthetic data. A synthetic data software – such as our open source Synthetic Data Vault (SDV) – uses machine learning to learn patterns from real patients, and then creates synthetic patients.\n\nThe SDV discovers neighborhoods at a variety of levels in the form of trends. It's able to learn overall trends (for all patients) as well as trends that are unique to a variable (such as Variant X). For example:\n\n * For all patients, a higher age corresponds to a greater risk of dementia and a higher BMI corresponds to a greater risk of diabetes.\n * Variant X patients tend to be older, while Omicron patients tend to be younger.\n * Etc.\n\nAs a result, synthetic patients have some variation – but the data still makes sense in context. An example table of SDV-generated patients is shown below.\n\nThis is the middle solution we were looking for: Synthetic data won't cause overfitting and is less noisy than randomization. The best part is that there are multiple synthetic data techniques and settings available in the SDV, providing flexibility and tradeoffs.\n\n\nTakeaways\n\nIn this article, we explored imbalanced datasets. It is common to have an imbalanced dataset due to rare events – which becomes a problem if those rare events are important for your project.\n\nFixing the imbalance problem requires a careful tradeoff between overfitting data and creating noisy data. Synthetic data is a compelling solution that achieves a middle ground by discovering neighborhoods of similar data. This allows you to realistically fix the imbalance without resorting to either extreme.\n\nAre you interested in label balancing? Have you already explored using the SDV for this problem? Drop us a comment below!","html":"<p>A dataset can unlock many doors for your organization, helping with everything from predictive forecasting to data-driven decision making. But in some situations, you may not have all of the data that you need. A common scenario is having too few data points, which can lead to an imbalance of variables.</p><p>In this article, we'll take a closer look at this scenario. We'll recap why this can be a problem for your projects and walk you through some possible solutions. We'll end by explaining why <strong>synthetic data</strong><em><strong> </strong></em>might be especially useful for overcoming this challenge.</p><h3 id=\"why-is-it-a-problem-to-have-a-data-imbalance\">Why is it a problem to have a data imbalance?</h3><p>At a basic level, data is a record of events, and a <strong>data</strong> <strong>imbalance</strong> happens when some events occur much less frequently than others. For example:</p><ul><li>In healthcare, cancer occurs less frequently than diabetes</li><li>In finance, it's rare to see fraudulent credit card charges</li><li>In local government, it's rare to have a day with a major natural disaster such as a fire</li></ul><p>A natural data imbalance isn't inherently a problem, but it can become one if the rare events are important. As an example, let's assume you're working at a hospital that is treating COVID patients. One day a new COVID variant – let's call it Variant X – appears in the population. Since it is so new, it currently occurs very rarely (&lt;2.5% of the time). This leads to a data imbalance for this COVID variant, as illustrated below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2023/01/Original-Dataset--2-.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2023/01/Original-Dataset--2-.png 600w, https://sdv.ghost.io/content/images/size/w1000/2023/01/Original-Dataset--2-.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2023/01/Original-Dataset--2-.png 1600w, https://sdv.ghost.io/content/images/2023/01/Original-Dataset--2-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>A hypothetical dataset of COVID patients. Each patient has multiple variables – Age, BMI, etc. The variable \"COVID Variant\" has an imbalance: Variant X is far less common than Omicron.</em></figcaption></figure><p>This imbalance is a problem when it's critical to account for Variant X. For example, you may want to build a predictive model for who is most likely to be hospitalized. If you use the data as-is, your model may only consider Omicron (the majority) and treat Variant X as an outlier. This can lead to poor predictions – and bad planning – because Variant X may soon become the dominant strain.</p><h3 id=\"using-data-augmentation-to-fix-imbalances\">Using Data Augmentation to Fix Imbalances</h3><p>In an ideal world, your data would include more patients with Variant X. But until then, you need to find a solution that will allow you to produce reasonable predictions. What if you create some artificial patient data for the sake of making a robust predictive model? </p><p>Let's assume you have data for 1,000 COVID hospital patients, 975 with the Omicron variant and 25 with Variant X. If you can create 950 additional artificial Variant X patients, then you can create an evenly-balanced dataset. This process is illustrated below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2023/01/Label-Balancing--2-.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2023/01/Label-Balancing--2-.png 600w, https://sdv.ghost.io/content/images/size/w1000/2023/01/Label-Balancing--2-.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2023/01/Label-Balancing--2-.png 1600w, https://sdv.ghost.io/content/images/2023/01/Label-Balancing--2-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>You can balance your data by creating artificial patients who all have Variant X. When you combine the new patients with the existing ones, you will have a balanced dataset, with 50% Omicron and 50% Variant X.</em></figcaption></figure><p>You may be skeptical because there are only 25 patients with Variant X to begin with. How can we reasonably produce 950 more based on that? As usual with data science, the devil is in the details. Let's go through some approaches to see what works.</p><h3 id=\"attempt-1-oversampling\">Attempt #1: Oversampling</h3><p>Your first instinct may be to take the existing 25 Variant X patients and weigh them more heavily. One easy way to achieve this: You can duplicate each original patient 40 times to get 1,000 patients.</p><p>In data science, this is known as <a href=\"https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis\">oversampling</a>. Sometimes, this is done programmatically, sampling patients (with replacement) as many times as needed. Other times, this can be achieved using mathematical formulas to provide weights. An illustration is shown below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2023/01/Oversampling--2-.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2023/01/Oversampling--2-.png 600w, https://sdv.ghost.io/content/images/size/w1000/2023/01/Oversampling--2-.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2023/01/Oversampling--2-.png 1600w, https://sdv.ghost.io/content/images/2023/01/Oversampling--2-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>One way to fix an imbalance is by oversampling your data. You can manually duplicate the rows (shown here), sample them with replacement or use mathematics to weigh them more heavily.</em></figcaption></figure><p>With oversampling, Variant X is no longer rare, so your model cannot ignore it. But if you actually use this data, your project may not be successful. Your model may confidently predict that all Variant X patients must be over the age of 50. But this is not necessarily right – just because the existing patients had these characteristics doesn't mean Variant X patients always will.</p><p>The mistake was over-emphasizing the same set of patients, making the model more likely to create strong claims. This is commonly referred to as <a href=\"https://en.wikipedia.org/wiki/Overfitting\">overfitting the data</a>: The model over-emphasizes the importance of a small number of records, and makes blanket predictions that lack nuance.</p><h3 id=\"attempt-2-randomizing\">Attempt #2: Randomizing</h3><p>To avoid the problems that come with oversampling, let's explore the opposite direction for argument's sake: What if we created artificial Variant X patients by choosing variables completely at random? An example is illustrated below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2023/01/Randomizing--2-.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2023/01/Randomizing--2-.png 600w, https://sdv.ghost.io/content/images/size/w1000/2023/01/Randomizing--2-.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2023/01/Randomizing--2-.png 1600w, https://sdv.ghost.io/content/images/2023/01/Randomizing--2-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Another way to fix an imbalance problem is by using randomization. You can create artificial Variant X patients by selecting the other variables (Age, BMI, etc.) at random.</em></figcaption></figure><p>Randomization avoids overfitting because no patient is repeated. But this approach introduces problems of its own: You may find that the data doesn't make sense anymore. The example data above highlights some problems that can arise. We see an artificial 23-year-old patient with dementia, and many diabetic patients with low BMIs. In the medical world, these events are not likely and indicate that there is a problem with the data.</p><p>These inconsistencies may (rightfully) dissuade you from using randomization. Since random data lacks patterns, a model will not be able to draw conclusions from it. In data science, we call this a problem of <em>noisiness</em>. <a href=\"https://en.wikipedia.org/wiki/Noisy_data\">Noisy data</a> has too many random combinations to produce any useful learnings.</p><h3 id=\"a-better-solution-defining-neighborhoods\">A Better Solution: Defining Neighborhoods</h3><p>So far, we've seen attempts at extreme ends.</p><ol><li>Oversampling will emphasize one set of patients, leading to an overfit model</li><li>Randomizing will make the dataset noisy, precluding useful conclusions</li></ol><p>The solution we need falls somewhere in the middle: We'd like to <em>loosely</em> base the artificial patients on the real ones. This is related to the data science concept of <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\">neighborhoods</a>. Drawing a <strong>neighborhood</strong> around some patients identifies general commonalities between them – without setting any strict rules. For example, Variant X patients <em>may</em> be more likely to have had a known exposure, but it's not guaranteed. Note that there is no exact definition for a neighborhood. It can change based on our assumptions and how broad we want to make it.</p><p>Once we know a neighborhood, we can create artificial patients that are inside it. These patients won't be exactly the same as the existing ones, but they won't have completely random values either.</p><p><strong>Synthetic Data for Label Balancing</strong></p><p>A compelling solution for discovering neighborhoods is <strong>synthetic data</strong>. A synthetic data<strong> </strong>software – such as our open source <a href=\"https://sdv.dev/\">Synthetic Data Vault</a> (SDV) – uses machine learning to learn patterns from real patients, and then creates synthetic patients.</p><p>The SDV discovers neighborhoods at a variety of levels in the form of trends. It's able to learn overall trends (for all patients) as well as trends that are unique to a variable (such as Variant X). For example:</p><ul><li>For all patients, a higher age corresponds to a greater risk of dementia and a higher BMI corresponds to a greater risk of diabetes.</li><li>Variant X patients tend to be older, while Omicron patients tend to be younger.</li><li>Etc.</li></ul><p>As a result, synthetic patients have some variation – but the data still makes sense in context. An example table of SDV-generated patients is shown below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2023/01/Synthesizing--2-.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2023/01/Synthesizing--2-.png 600w, https://sdv.ghost.io/content/images/size/w1000/2023/01/Synthesizing--2-.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2023/01/Synthesizing--2-.png 1600w, https://sdv.ghost.io/content/images/2023/01/Synthesizing--2-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>A software like the SDV generates synthetic patients with Variant X. The synthetic patients are not exact duplicates of the original, but they aren't completely random either.</em></figcaption></figure><p>This is the middle solution we were looking for: Synthetic data won't cause overfitting and is less noisy than randomization. The best part is that there are multiple synthetic data techniques and settings available in the SDV, providing flexibility and tradeoffs.</p><h3 id=\"takeaways\">Takeaways</h3><p>In this article, we explored imbalanced datasets. It is common to have an imbalanced dataset due to rare events – which becomes a problem if those rare events are important for your project.</p><p>Fixing the imbalance problem requires a careful tradeoff between overfitting data and creating noisy data. <strong>Synthetic data</strong> is a compelling solution that achieves a middle ground by discovering neighborhoods of similar data. This allows you to realistically fix the imbalance without resorting to either extreme.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2023/01/Data-Creation-Spectrum--2-.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"1000\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2023/01/Data-Creation-Spectrum--2-.png 600w, https://sdv.ghost.io/content/images/size/w1000/2023/01/Data-Creation-Spectrum--2-.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2023/01/Data-Creation-Spectrum--2-.png 1600w, https://sdv.ghost.io/content/images/2023/01/Data-Creation-Spectrum--2-.png 2000w\" sizes=\"(min-width: 720px) 720px\"><figcaption><em>Synthetic data is a solution that balances the extremes of overfitting the data and creating noisy data.</em></figcaption></figure><p>Are you interested in label balancing? Have you already explored using the SDV for this problem? Drop us a comment below!</p>","url":"https://sdv.ghost.io/synthetic-label-balancing/","canonical_url":"https://datacebo.com/blog/synthetic-label-balancing","uuid":"2ca67664-3ca9-4710-942e-5536ec5f2dc8","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"63b4711dac52ed003d6a1744","reading_time":6}},{"node":{"id":"Ghost__Post__61927ca167598b003b3d944a","title":"Software Testing: Synthetic data changes the game","slug":"fake-to-synthetic-ml","featured":true,"feature_image":"https://sdv.ghost.io/content/images/2021/11/Article-13.png","excerpt":"Creating fake data is an old concept -- but machine learning is a whole new ballgame. Learn about why ML is a key ingredient to synthetic data.","custom_excerpt":"Creating fake data is an old concept -- but machine learning is a whole new ballgame. Learn about why ML is a key ingredient to synthetic data.","visibility":"public","created_at_pretty":"15 November, 2021","published_at_pretty":"16 November, 2021","updated_at_pretty":"01 October, 2022","created_at":"2021-11-15T10:28:33.000-05:00","published_at":"2021-11-16T11:33:56.000-05:00","updated_at":"2022-10-01T01:14:30.000-04:00","meta_title":"From fake to synthetic data: Machine learning changes the game","meta_description":"Creating fake data is an old concept -- but machine learning is a whole new ballgame. Learn about why ML is a key ingredient to synthetic data.","og_description":null,"og_image":null,"og_title":null,"twitter_description":"Creating fake data is an old concept -- but machine learning is a whole new ballgame. Learn about why ML is a key ingredient to synthetic data.","twitter_image":null,"twitter_title":"From fake to synthetic data: Machine learning changes the game","authors":[{"name":"Neha Patki","slug":"neha","bio":"Neha first created the SDV for her Master's thesis at MIT and also has experience in Product Management from Google. She is excited to use her expertise to build a great user experience at DataCebo.","profile_image":"https://sdv.ghost.io/content/images/2021/05/Neha_Patki--1-.jpg","twitter":"@n4atki","facebook":null,"website":"https://www.linkedin.com/in/nehapatki/"}],"primary_author":{"name":"Neha Patki","slug":"neha","bio":"Neha first created the SDV for her Master's thesis at MIT and also has experience in Product Management from Google. She is excited to use her expertise to build a great user experience at DataCebo.","profile_image":"https://sdv.ghost.io/content/images/2021/05/Neha_Patki--1-.jpg","twitter":"@n4atki","facebook":null,"website":"https://www.linkedin.com/in/nehapatki/"},"primary_tag":{"name":"Applications","slug":"applications","description":"This blog focuses on the value that synthetic data brings to solving enterprise problems. Synthetic data can be used to test applications, develop unbiased AI models and more. Explore new use cases, concepts and case studies.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"},"tags":[{"name":"Applications","slug":"applications","description":"This blog focuses on the value that synthetic data brings to solving enterprise problems. Synthetic data can be used to test applications, develop unbiased AI models and more. Explore new use cases, concepts and case studies.","feature_image":null,"meta_description":null,"meta_title":null,"visibility":"public"}],"plaintext":"Data is a great source of information. Real data — which is based on\nobservations of real-world phenomena like weather, movements on a factory floor\nor the activities of a user base — can help us notice trends, increase business\nefficiency and solve problems. \n\nBut data can be helpful even if it isn’t real. This data, sometimes called fake\nor test data, doesn’t come directly from real-world observations, but is instead\nartificially crafted by a human or machine. The latest and most complex\niteration of this data type — what we call synthetic data — builds on previous\nwork done in this space. \n\nIn this article, we'll go through the history of fake data. By the end, you'll\nbe able to answer the following questions:\n\n * What were the original motivations and tools for manually creating data?\n * What differentiates synthetic data from other types of fake data?\n * What role does machine learning play in generating synthetic data?\n\nThe Dawn of Fake Data: Test Data Management\nOne group of people has worked with fake data for a long time: software\nengineers. They need data in order to test the systems they build, and the real\nstuff isn't always usable (for example, due to privacy). \n\nLet's pretend it's the early 2000s, and you're an IT professional working at a\nbank. You're responsible for the software that updates account balances after\neach transaction. You'd like to test this software before putting it into\nproduction. What do you do?\n\nMost likely, you'll come up with a few test scenarios to ensure that your\nfunctionality — updating the balance — can properly handle a variety of inputs.\n\nThis table shows a few scenarios you may use to test your system. In these\nscenarios, you're testing how a monetary transfer of $20 changes the balance in\ndifferent accounts.Notice that in order to create these scenarios, you had to\ngenerate data: various starting balances ($500, $20, $10) as well as a transfer\namount ($20). This is an early version of using fake data in order to test your\nsoftware!\n\nUsing Tools for Manual Creation\n\nNow let's fast forward in time. Over the years, your software has gotten even\nmore complex, and you're constantly adding new functionalities. For example,\nmaybe you start allowing transfers with foreign currency. \n\nYou need to test these functionalities before you roll them out. To save time,\nyou might end up using -- or creating -- a tool that allows you to generate and\nmanage fake data for testing. \n\nThe simplest tool may be a basic permutation, as illustrated below.\n\nA simple manual test data generation tool that uses permutations. The resulting\nscenarios -- with different starting balances, transfer amounts and transfer\ncurrencies -- are outputted as a data table.A more sophisticated tool might\nallow you greater control over the rules the data must follow. It will also\nallow you to create more columns as your functionalities increase. For example,\nmaybe the bank now offers two different account types: Premium and Normal. \n\nNow you need a test data generation tool that can handle all of these variables\nand come out with something like this:\n\nA more sophisticated test data tool will allow you to specify rules manually. It\nwill follow them to generate test data.Many test data management tools use\nsophisticated logic to precisely create these data columns and their values. But\nthe rules they use are manually written, and rely on human intuition and domain\nknowledge. For example:\n\n * Account type = Premium 10% of the time and Normal 90% of the time\n * Starting balance is between $10,000 and $250,000 if Account type = Premium\n   or between -$1,000 and $10,000 if Account type = Normal\n * Transfer amount follows a bell curve with a mean of $7,500 and standard\n   deviation of $1,000\n * Etc.\n\nThere are downsides to this manual approach. It takes time and effort to come up\nwith these rules, to keep track of them, and to update them as your application\nchanges.\n\nAdding Machine Learning\nAdopting machine learning (ML) opens up entirely new avenues in data generation.\nIn the process, it gets rid of some of these downsides.\n\nAt a high level, ML-based software (such as the Synthetic Data Vault\n[https://sdv.dev/blog/intro-to-sdv/]) works in three steps:\n\n 1. The user inputs real data into the ML software\n 2. The ML software automatically learns patterns in the data\n 3. The software outputs data that contains those patterns\n\nLet's go back to our banking example to see how this works. It's now 2021 and\nyou're using the SDV [https://sdv.dev/] to generate your test data. You input\nall the transactions your bank has handled in the last week. \n\nAfter modeling, the SDV outputs entirely new data that looks and behaves like\nthe original. An illustration of this is shown below.\n\nWith ML tools (like the SDV), you input real data into the software. The\nsoftware then learns patterns from the data and outputs data that matches those\npatterns.Notice that the output data contains many of the same properties as the\noriginal. The model learned all of the following information:\n\n * Ranges & Categories. Transfers range from $5K to $10K. Bank accounts can be\n   either premium or normal. Etc.\n * Shapes. 10% of accounts are premium. Transfers follow a bell curve\n   distribution with a mean of $7,500 and a standard deviation of $1,000. Etc.\n * Correlations. Premium bank accounts tend to have higher balances ($10K to\n   $250K) than normal accounts (-$1K to $10K).\n\nIn other words: while the old test data management tools required you to\nmanually come up with rules, ML-based tools learn these rules automatically. \nMoreover, they can learn new information. For example, the ML picked upon a\ncouple of extra correlations:\n\n * Premium accounts are more likely to transfer foreign currency.\n * Normal accounts are more likely to be overdrawn (transfer more than their\n   current balance).\n\nUsing an ML-based data generation tool will help you ensure that your software\nis robust against these typical cases. And while manual data generation tools\ngenerate fake data, ML-based approaches generate what we call synthetic data.\n\nAsk whether you had to input any real data or rules. Based on this, you'll know\nwhether you are dealing with synthetic data or fake data.Benefits of Synthetic\nData\n\nThere are some clear advantages to using synthetic data over fake data,\nespecially in software testing. Below, we've detailed a few.\n\n * Saves time with automation. Because ML automatically learns patterns from the\n   real data, there is no need to spend a lot of time coming up with and\n   inputting rules. ML learns rules that you may even miss.\n * Is usable by non-experts. Realistic fake data can only be generated by domain\n   experts, who know the precise rules governing the dataset. However, anyone\n   can generate synthetic data. All they have to do is input the real data and\n   the ML software takes care of the rest!\n * Increases adaptability. Applications and data will inevitably change over\n   time. It's easy to update synthetic data as this happens, simply by\n   retraining the ML model with newer data.\n\nBenefits of synthetic data expand beyond software testing. The SDV Community is\nusing synthetic data for an ever-increasing variety of tasks, including machine\nlearning development, de-biasing datasets and scenario planning.\n\nKey Takeaways\nIn this article, we surveyed numerous ways of creating and using data  that is\nnot real. In particular, we learned that:\n\n * Creating fake data is not a novel concept. Older generations of tools will\n   output fake data when given an explicit list of rules. This is especially\n   useful for software testing.\n * Adding ML to this process is a newer evolution. Users input real data into\n   the ML model, and it's able to automatically infer the rules. Data generated\n   using ML-based systems is known as synthetic data.\n * Synthetic data's key advantages include its automation and adaptability. The\n   uses of synthetic data expand beyond software testing.\n\nIn future articles, we'll put ML models to the test! We'll uncover their\nstrengths and weaknesses, and guide you through getting the most from synthetic\ndata using the Synthetic Data Vault.","html":"<p>Data is a great source of information. Real data — which is based on observations of real-world phenomena like weather, movements on a factory floor or the activities of a user base — can help us notice trends, increase business efficiency and solve problems. </p><p>But data can be helpful even if it isn’t real. This data, sometimes called fake or test data, doesn’t come directly from real-world observations, but is instead artificially crafted by a human or machine. The latest and most complex iteration of this data type — what we call synthetic data — builds on previous work done in this space. </p><p>In this article, we'll go through the history of fake data. By the end, you'll be able to answer the following questions:</p><ul><li>What were the original motivations and tools for manually creating data?</li><li>What differentiates synthetic data from other types of fake data?</li><li>What role does machine learning play in generating synthetic data?</li></ul><h3 id=\"the-dawn-of-fake-data-test-data-management\">The Dawn of Fake Data: Test Data Management</h3><p>One group of people has worked with fake data for a long time: software engineers. They need data in order to test the systems they build, and the real stuff isn't always usable (for example, due to privacy). </p><p>Let's pretend it's the early 2000s, and you're an IT professional working at a bank. You're responsible for the software that updates account balances after each transaction. You'd like to test this software before putting it into production. What do you do?</p><p>Most likely, you'll come up with a few test scenarios to ensure that your functionality — updating the balance — can properly handle a variety of inputs.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2021/11/Article-09--1-.png\" class=\"kg-image\" alt=\"This table shows a few scenarios you may use to test your system. In these scenarios, you're testing how a monetary transfer of $20 changes the balance in different accounts.\" loading=\"lazy\" width=\"2000\" height=\"541\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/11/Article-09--1-.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/11/Article-09--1-.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/11/Article-09--1-.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2021/11/Article-09--1-.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>This table shows a few scenarios you may use to test your system. In these scenarios, you're testing how a monetary transfer of $20 changes the balance in different accounts.</figcaption></figure><p>Notice that in order to create these scenarios, you had to generate data: various starting balances ($500, $20, $10) as well as a transfer amount ($20). This is an early version of using fake data in order to test your software!</p><p><strong>Using Tools for Manual Creation</strong></p><p>Now let's fast forward in time. Over the years, your software has gotten even more complex, and you're constantly adding new functionalities. For example, maybe you start allowing transfers with foreign currency. </p><p>You need to test these functionalities before you roll them out. To save time, you might end up using -- or creating -- a tool that allows you to generate and manage fake data for testing. </p><p>The simplest tool may be a basic permutation, as illustrated below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2021/11/Article-07-2.png\" class=\"kg-image\" alt=\"A simple manual test data generation tool that uses permutations. The resulting scenarios -- with different starting balances, transfer amounts and transfer currencies -- are outputted as a data table.\" loading=\"lazy\" width=\"1723\" height=\"809\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/11/Article-07-2.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/11/Article-07-2.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/11/Article-07-2.png 1600w, https://sdv.ghost.io/content/images/2021/11/Article-07-2.png 1723w\" sizes=\"(min-width: 720px) 720px\"><figcaption>A simple manual test data generation tool that uses permutations. The resulting scenarios -- with different starting balances, transfer amounts and transfer currencies -- are outputted as a data table.</figcaption></figure><p>A more sophisticated tool might allow you greater control over the rules the data must follow. It will also allow you to create more columns as your functionalities increase. For example, maybe the bank now offers two different account types: Premium and Normal. </p><p>Now you need a test data generation tool that can handle all of these variables and come out with something like this:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2021/11/Article-11.png\" class=\"kg-image\" alt=\"A more sophisticated test data tool will allow you to specify rules manually. It will follow them to generate test data.\" loading=\"lazy\" width=\"1955\" height=\"655\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/11/Article-11.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/11/Article-11.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/11/Article-11.png 1600w, https://sdv.ghost.io/content/images/2021/11/Article-11.png 1955w\" sizes=\"(min-width: 720px) 720px\"><figcaption>A more sophisticated test data tool will allow you to specify rules manually. It will follow them to generate test data.</figcaption></figure><p>Many test data management tools use sophisticated logic to precisely create these data columns and their values. But the rules they use are manually written, and rely on human intuition and domain knowledge. For example:</p><ul><li>Account type = Premium 10% of the time and Normal 90% of the time</li><li>Starting balance is between $10,000 and $250,000 if Account type = Premium<br>or between -$1,000 and $10,000 if Account type = Normal</li><li>Transfer amount follows a bell curve with a mean of $7,500 and standard deviation of $1,000</li><li>Etc.</li></ul><p>There are downsides to this manual approach. It takes time and effort to come up with these rules, to keep track of them, and to update them as your application changes.</p><h3 id=\"adding-machine-learning\">Adding Machine Learning</h3><p>Adopting machine learning (ML) opens up entirely new avenues in data generation. In the process, it gets rid of some of these downsides.</p><p>At a high level, ML-based software (such as the <a href=\"https://sdv.dev/blog/intro-to-sdv/\">Synthetic Data Vault</a>) works in three steps:</p><ol><li>The user inputs real data into the ML software</li><li>The ML software automatically learns patterns in the data</li><li>The software outputs data that contains those patterns</li></ol><p>Let's go back to our banking example to see how this works. It's now 2021 and you're using <a href=\"https://sdv.dev/\">the SDV</a> to generate your test data. You input all the transactions your bank has handled in the last week. </p><p>After modeling, the SDV outputs entirely new data that looks and behaves like the original. An illustration of this is shown below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2021/11/Article-10.png\" class=\"kg-image\" alt=\"With ML tools (like the SDV), you input real data into the software. The software then learns patterns from the data and outputs data that matches those patterns.\" loading=\"lazy\" width=\"2000\" height=\"516\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/11/Article-10.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/11/Article-10.png 1000w, https://sdv.ghost.io/content/images/size/w1600/2021/11/Article-10.png 1600w, https://sdv.ghost.io/content/images/size/w2400/2021/11/Article-10.png 2400w\" sizes=\"(min-width: 720px) 720px\"><figcaption>With ML tools (like the SDV), you input real data into the software. The software then learns patterns from the data and outputs data that matches those patterns.</figcaption></figure><p>Notice that the output data contains many of the same properties as the original. The model learned all of the following information:</p><ul><li><strong>Ranges &amp; Categories.</strong> Transfers range from $5K to $10K. Bank accounts can be either premium or normal. Etc.</li><li><strong>Shapes.</strong> 10% of accounts are premium. Transfers follow a bell curve distribution with a mean of $7,500 and a standard deviation of $1,000. Etc.</li><li><strong>Correlations.</strong> Premium bank accounts tend to have higher balances ($10K to $250K) than normal accounts (-$1K to $10K).</li></ul><p>In other words: <strong>while the old test data management tools required you to manually come up with rules, ML-based tools learn these rules automatically.</strong> <strong> </strong>Moreover, they can learn new information. For example, the ML picked upon a couple of extra correlations:</p><ul><li>Premium accounts are more likely to transfer foreign currency.</li><li>Normal accounts are more likely to be overdrawn (transfer more than their current balance).</li></ul><p>Using an ML-based data generation tool will help you ensure that your software is robust against these typical cases. And while manual data generation tools generate fake data, <strong>ML-based approaches generate what we call synthetic data.</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://sdv.ghost.io/content/images/2021/11/Article-08-1.png\" class=\"kg-image\" alt=\"Ask whether you had to input any real data or rules. Based on this, you'll know whether you are dealing with synthetic data or fake data.\" loading=\"lazy\" width=\"1574\" height=\"419\" srcset=\"https://sdv.ghost.io/content/images/size/w600/2021/11/Article-08-1.png 600w, https://sdv.ghost.io/content/images/size/w1000/2021/11/Article-08-1.png 1000w, https://sdv.ghost.io/content/images/2021/11/Article-08-1.png 1574w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Ask whether you had to input any real data or rules. Based on this, you'll know whether you are dealing with synthetic data or fake data.</figcaption></figure><p><strong>Benefits of Synthetic Data</strong></p><p>There are some clear advantages to using synthetic data over fake data, especially in software testing. Below, we've detailed a few.</p><ul><li><strong>Saves time with automation.</strong> Because ML automatically learns patterns from the real data, there is no need to spend a lot of time coming up with and inputting rules. ML learns rules that you may even miss.</li><li><strong>Is usable by non-experts. </strong>Realistic fake data can only be generated by domain experts, who know the precise rules governing the dataset. However, anyone can generate synthetic data. All they have to do is input the real data and the ML software takes care of the rest!</li><li><strong>Increases adaptability. </strong>Applications and data will inevitably change over time. It's easy to update synthetic data as this happens, simply by retraining the ML model with newer data.</li></ul><p>Benefits of synthetic data expand beyond software testing. The SDV Community is using synthetic data for an ever-increasing variety of tasks, including machine learning development, de-biasing datasets and scenario planning.</p><h3 id=\"key-takeaways\">Key Takeaways</h3><p>In this article, we surveyed numerous ways of creating and using data  that is not real. In particular, we learned that:</p><ul><li>Creating fake data is not a novel concept. Older generations of tools will output fake data when given an explicit list of rules. This is especially useful for software testing.</li><li>Adding ML to this process is a newer evolution. Users input real data into the ML model, and it's able to automatically infer the rules. Data generated using ML-based systems is known as <strong>synthetic data</strong>.</li><li>Synthetic data's key advantages include its automation and adaptability. The uses of synthetic data expand beyond software testing.</li></ul><p>In future articles, we'll put ML models to the test! We'll uncover their strengths and weaknesses, and guide you through getting the most from synthetic data using the Synthetic Data Vault.</p>","url":"https://sdv.ghost.io/fake-to-synthetic-ml/","canonical_url":null,"uuid":"b2a72ccc-d24f-42ad-90fb-23d0017e2529","page":null,"codeinjection_foot":null,"codeinjection_head":null,"codeinjection_styles":null,"comment_id":"61927ca167598b003b3d944a","reading_time":6}}]}},"pageContext":{"slug":"applications","pageNumber":0,"humanPageNumber":1,"skip":0,"limit":12,"numberOfPages":1,"previousPagePath":"","nextPagePath":""}},"staticQueryHashes":["2061773391","2358152166","2362887240","2439066133","2561578252","2657115718","2731221146","2839364760","4145280475"]}